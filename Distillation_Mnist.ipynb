{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "u4d0UVthpGj_",
        "kY-uM2ZcpIP3",
        "LteUmL9HSxFe",
        "8X5oU7tqlAng",
        "P01gbJKMpTId",
        "_Nf7CEXcpv35",
        "vCiqbi2K3_pg",
        "oLKj6NSGntxp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hinton Version - \"Vanilla\" Model"
      ],
      "metadata": {
        "id": "KUWit6a0ouqU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture - always execute this"
      ],
      "metadata": {
        "id": "u4d0UVthpGj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################## 1 ########################\n",
        "\n",
        "%%writefile models.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TeacherNet(nn.Module):\n",
        "    \"\"\"Large teacher network: 2 hidden layers of 1200 ReLU units\"\"\"\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 1200)\n",
        "        self.fc2 = nn.Linear(1200, 1200)\n",
        "        self.fc3 = nn.Linear(1200, 10)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc3(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "\n",
        "class StudentNet(nn.Module):\n",
        "    \"\"\"Small student network: 2 hidden layers of 800 ReLU units\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 800)\n",
        "        self.fc2 = nn.Linear(800, 800)\n",
        "        self.fc3 = nn.Linear(800, 10)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "\n",
        "def distillation_loss(student_logits, teacher_soft_targets, hard_targets,\n",
        "                      temperature, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Combined loss for distillation.\n",
        "\n",
        "    Args:\n",
        "        student_logits: Raw outputs from student model\n",
        "        teacher_soft_targets: Soft probabilities from teacher (at temperature T)\n",
        "        hard_targets: Ground truth labels\n",
        "        temperature: Temperature for distillation\n",
        "        alpha: Weight for hard target loss (1-alpha is weight for soft targets)\n",
        "\n",
        "    Returns:\n",
        "        Combined loss\n",
        "    \"\"\"\n",
        "    # Soft target loss: KL divergence between student and teacher (both at temperature T)\n",
        "    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n",
        "    soft_loss = F.kl_div(student_soft, teacher_soft_targets, reduction='batchmean')\n",
        "\n",
        "    # Scale by T^2 as per paper (gradients scale as 1/T^2)\n",
        "    soft_loss = soft_loss * (temperature ** 2)\n",
        "\n",
        "    # Hard target loss: Standard cross-entropy (at temperature 1)\n",
        "    hard_loss = F.cross_entropy(student_logits, hard_targets)\n",
        "\n",
        "    # Weighted combination\n",
        "    return alpha * hard_loss + (1 - alpha) * soft_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy1NDLItowFu",
        "outputId": "b345037e-83bb-419f-cf8a-d4c479b00f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################## 2 ########################\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from models import TeacherNet, StudentNet, distillation_loss\n",
        "\n",
        "\n",
        "def train_teacher(model, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train the large teacher model with dropout regularization\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, logits = model(data)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Teacher Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_student_normal(model, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train student model normally (baseline)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, logits = model(data)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Student (normal) Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_student_distilled(student, teacher, train_loader, temperature=20,\n",
        "                           alpha=0.1, epochs=10, lr=0.001):\n",
        "    \"\"\"Train student model using knowledge distillation\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()  # Teacher is frozen\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher's soft targets at temperature T\n",
        "            with torch.no_grad():\n",
        "                teacher_soft_targets, _ = teacher(data, temperature=temperature)\n",
        "\n",
        "            # Get student outputs\n",
        "            _, student_logits = student(data)\n",
        "\n",
        "            # Compute distillation loss\n",
        "            loss = distillation_loss(student_logits, teacher_soft_targets,\n",
        "                                    target, temperature, alpha)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Student (distilled T={temperature}) Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    \"\"\"Evaluate model accuracy\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs, _ = model(data, temperature=1.0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    errors = total - correct\n",
        "    return accuracy, errors\n"
      ],
      "metadata": {
        "id": "Lcmm7p4mpKpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3 of paper replicated - NO NEED TO EXECUTE"
      ],
      "metadata": {
        "id": "kY-uM2ZcpIP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version 1: T = 20, Dropout = 0.1, Epochs = 20, lr = 0.001, batchsize = 512"
      ],
      "metadata": {
        "id": "cA9HHECVvs5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Rest of your code...\n",
        "\n",
        "    dropout = 0.2\n",
        "    epochs = 20\n",
        "    temp = 20\n",
        "    lr = 0.001\n",
        "\n",
        "\n",
        "\n",
        "    # Data loading with jittering (up to 2 pixels in any direction)\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomAffine(degrees=0, translate=(2/28, 2/28)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True,\n",
        "                         num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"MNIST Knowledge Distillation Experiment (Section 3)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 1. Train large teacher network (1200-1200 units with dropout)\n",
        "    print(\"\\n[1/4] Training Teacher Network (2x1200 ReLU units + dropout)...\")\n",
        "    teacher = TeacherNet(dropout_rate=dropout)\n",
        "    teacher = train_teacher(teacher, train_loader, epochs=epochs, lr=lr)\n",
        "    teacher_acc, teacher_err = evaluate(teacher, test_loader)\n",
        "    print(f\"✓ Teacher: {teacher_acc:.2f}% accuracy ({teacher_err} errors)\")\n",
        "\n",
        "    # 2. Train small student network normally (800-800 units, no regularization)\n",
        "    print(\"\\n[2/4] Training Student Network Normally (2x800 ReLU units, no regularization)...\")\n",
        "    student_normal = StudentNet()\n",
        "    student_normal = train_student_normal(student_normal, train_loader, epochs=epochs, lr=lr)\n",
        "    student_normal_acc, student_normal_err = evaluate(student_normal, test_loader)\n",
        "    print(f\"✓ Student (normal): {student_normal_acc:.2f}% accuracy ({student_normal_err} errors)\")\n",
        "\n",
        "    # 3. Train small student network with distillation (T=20)\n",
        "    print(\"\\n[3/4] Training Student Network with Distillation (T=20, alpha=0.1)...\")\n",
        "    student_distilled = StudentNet()\n",
        "    student_distilled = train_student_distilled(student_distilled, teacher, train_loader,\n",
        "                                                temperature=temp, alpha=0.1, epochs=epochs, lr=lr)\n",
        "    student_distilled_acc, student_distilled_err = evaluate(student_distilled, test_loader)\n",
        "    print(f\"✓ Student (distilled): {student_distilled_acc:.2f}% accuracy ({student_distilled_err} errors)\")\n",
        "\n",
        "    # 4. Results summary\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"RESULTS SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Teacher (2x1200 + dropout):        {teacher_err:3d} test errors ({teacher_acc:.2f}%)\")\n",
        "    print(f\"Student normal (2x800):            {student_normal_err:3d} test errors ({student_normal_acc:.2f}%)\")\n",
        "    print(f\"Student distilled (2x800, T=20):   {student_distilled_err:3d} test errors ({student_distilled_acc:.2f}%)\")\n",
        "    print(\"\\nPaper reported:\")\n",
        "    print(\"Teacher:           67 test errors\")\n",
        "    print(\"Student normal:   146 test errors\")\n",
        "    print(\"Student distilled: 74 test errors\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 5. Quick inference speed test\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"INFERENCE SPEED TEST\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    import time\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    sample = next(iter(test_loader))[0][:1].to(device)  # Single sample\n",
        "\n",
        "    def count_params(model):\n",
        "        return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "    def benchmark(model, name):\n",
        "        model.eval()\n",
        "        params = count_params(model)\n",
        "        with torch.no_grad():\n",
        "            for _ in range(10): model(sample)  # Warm-up\n",
        "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "            start = time.time()\n",
        "            for _ in range(1000): model(sample)\n",
        "            if torch.cuda.is_available(): torch.cuda.synchronize()\n",
        "            ms = (time.time() - start) / 1000 * 1000\n",
        "        print(f\"{name:25s}: {ms:.3f} ms/sample  ({params:,} params)\")\n",
        "        return ms\n",
        "\n",
        "    t_time = benchmark(teacher, \"Teacher (2x1200)\")\n",
        "    s_time = benchmark(student_normal, \"Student (2x800)\")\n",
        "    d_time = benchmark(student_distilled, \"Distilled (2x800)\")\n",
        "\n",
        "    print(f\"\\nSpeedup: {t_time/s_time:.2f}x faster with {count_params(student_normal)/count_params(teacher)*100:.1f}% params\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "    # 6. save models architecture and weights\n",
        "\n",
        "    # Save models\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SAVING MODELS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    import os\n",
        "    save_dir = './saved_models'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': teacher.state_dict(),\n",
        "        'dropout_rate': dropout\n",
        "    }, f'{save_dir}/teacher_model.pth')\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': student_normal.state_dict(),\n",
        "    }, f'{save_dir}/student_normal_model.pth')\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': student_distilled.state_dict(),\n",
        "    }, f'{save_dir}/student_distilled_model.pth')\n",
        "\n",
        "    print(f\"✓ Models saved to {save_dir}/\")\n",
        "\n",
        "    # Download in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        import shutil\n",
        "\n",
        "        # Create zip file\n",
        "        shutil.make_archive('mnist_distillation_models', 'zip', save_dir)\n",
        "        files.download('mnist_distillation_models.zip')\n",
        "        print(\"✓ Models downloaded as mnist_distillation_models.zip!\")\n",
        "    except:\n",
        "        print(\"Not running in Colab - models saved locally only\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Et0Wa4sfPAGJ",
        "outputId": "23e8da3c-0949-4bdb-b869-8601eb24b1b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n",
            "============================================================\n",
            "MNIST Knowledge Distillation Experiment (Section 3)\n",
            "============================================================\n",
            "\n",
            "[1/4] Training Teacher Network (2x1200 ReLU units + dropout)...\n",
            "Teacher Epoch 1/20, Loss: 0.4231\n",
            "Teacher Epoch 2/20, Loss: 0.1569\n",
            "Teacher Epoch 3/20, Loss: 0.1245\n",
            "Teacher Epoch 4/20, Loss: 0.1110\n",
            "Teacher Epoch 5/20, Loss: 0.0983\n",
            "Teacher Epoch 6/20, Loss: 0.0941\n",
            "Teacher Epoch 7/20, Loss: 0.0893\n",
            "Teacher Epoch 8/20, Loss: 0.0833\n",
            "Teacher Epoch 9/20, Loss: 0.0791\n",
            "Teacher Epoch 10/20, Loss: 0.0774\n",
            "Teacher Epoch 11/20, Loss: 0.0757\n",
            "Teacher Epoch 12/20, Loss: 0.0712\n",
            "Teacher Epoch 13/20, Loss: 0.0743\n",
            "Teacher Epoch 14/20, Loss: 0.0718\n",
            "Teacher Epoch 15/20, Loss: 0.0706\n",
            "Teacher Epoch 16/20, Loss: 0.0677\n",
            "Teacher Epoch 17/20, Loss: 0.0647\n",
            "Teacher Epoch 18/20, Loss: 0.0678\n",
            "Teacher Epoch 19/20, Loss: 0.0680\n",
            "Teacher Epoch 20/20, Loss: 0.0640\n",
            "✓ Teacher: 98.95% accuracy (105 errors)\n",
            "\n",
            "[2/4] Training Student Network Normally (2x800 ReLU units, no regularization)...\n",
            "Student (normal) Epoch 1/20, Loss: 0.4459\n",
            "Student (normal) Epoch 2/20, Loss: 0.1508\n",
            "Student (normal) Epoch 3/20, Loss: 0.1103\n",
            "Student (normal) Epoch 4/20, Loss: 0.0905\n",
            "Student (normal) Epoch 5/20, Loss: 0.0824\n",
            "Student (normal) Epoch 6/20, Loss: 0.0730\n",
            "Student (normal) Epoch 7/20, Loss: 0.0642\n",
            "Student (normal) Epoch 8/20, Loss: 0.0622\n",
            "Student (normal) Epoch 9/20, Loss: 0.0568\n",
            "Student (normal) Epoch 10/20, Loss: 0.0536\n",
            "Student (normal) Epoch 11/20, Loss: 0.0490\n",
            "Student (normal) Epoch 12/20, Loss: 0.0449\n",
            "Student (normal) Epoch 13/20, Loss: 0.0456\n",
            "Student (normal) Epoch 14/20, Loss: 0.0437\n",
            "Student (normal) Epoch 15/20, Loss: 0.0383\n",
            "Student (normal) Epoch 16/20, Loss: 0.0405\n",
            "Student (normal) Epoch 17/20, Loss: 0.0392\n",
            "Student (normal) Epoch 18/20, Loss: 0.0368\n",
            "Student (normal) Epoch 19/20, Loss: 0.0356\n",
            "Student (normal) Epoch 20/20, Loss: 0.0323\n",
            "✓ Student (normal): 98.77% accuracy (123 errors)\n",
            "\n",
            "[3/4] Training Student Network with Distillation (T=20, alpha=0.1)...\n",
            "Student (distilled T=20) Epoch 1/20, Loss: 3.0224\n",
            "Student (distilled T=20) Epoch 2/20, Loss: 0.4318\n",
            "Student (distilled T=20) Epoch 3/20, Loss: 0.2225\n",
            "Student (distilled T=20) Epoch 4/20, Loss: 0.1559\n",
            "Student (distilled T=20) Epoch 5/20, Loss: 0.1278\n",
            "Student (distilled T=20) Epoch 6/20, Loss: 0.1099\n",
            "Student (distilled T=20) Epoch 7/20, Loss: 0.0963\n",
            "Student (distilled T=20) Epoch 8/20, Loss: 0.0900\n",
            "Student (distilled T=20) Epoch 9/20, Loss: 0.0812\n",
            "Student (distilled T=20) Epoch 10/20, Loss: 0.0750\n",
            "Student (distilled T=20) Epoch 11/20, Loss: 0.0709\n",
            "Student (distilled T=20) Epoch 12/20, Loss: 0.0681\n",
            "Student (distilled T=20) Epoch 13/20, Loss: 0.0629\n",
            "Student (distilled T=20) Epoch 14/20, Loss: 0.0599\n",
            "Student (distilled T=20) Epoch 15/20, Loss: 0.0598\n",
            "Student (distilled T=20) Epoch 16/20, Loss: 0.0563\n",
            "Student (distilled T=20) Epoch 17/20, Loss: 0.0547\n",
            "Student (distilled T=20) Epoch 18/20, Loss: 0.0516\n",
            "Student (distilled T=20) Epoch 19/20, Loss: 0.0514\n",
            "Student (distilled T=20) Epoch 20/20, Loss: 0.0482\n",
            "✓ Student (distilled): 98.85% accuracy (115 errors)\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Teacher (2x1200 + dropout):        105 test errors (98.95%)\n",
            "Student normal (2x800):            123 test errors (98.77%)\n",
            "Student distilled (2x800, T=20):   115 test errors (98.85%)\n",
            "\n",
            "Paper reported:\n",
            "Teacher:           67 test errors\n",
            "Student normal:   146 test errors\n",
            "Student distilled: 74 test errors\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "INFERENCE SPEED TEST\n",
            "============================================================\n",
            "Teacher (2x1200)         : 0.163 ms/sample  (2,395,210 params)\n",
            "Student (2x800)          : 0.133 ms/sample  (1,276,810 params)\n",
            "Distilled (2x800)        : 0.157 ms/sample  (1,276,810 params)\n",
            "\n",
            "Speedup: 1.23x faster with 53.3% params\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "SAVING MODELS\n",
            "============================================================\n",
            "✓ Models saved to ./saved_models/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7108a67c-edc4-4eec-b80d-75a51018947a\", \"mnist_distillation_models.zip\", 18498052)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Models downloaded as mnist_distillation_models.zip!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example of loading trained models' params and feeding samples through them"
      ],
      "metadata": {
        "id": "LteUmL9HSxFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from models import TeacherNet, StudentNet, distillation_loss\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# assuming new session so loading MNIST dataset from scratch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# data loading with jittering (up to 2 pixels in any direction)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=0, translate=(2/28, 2/28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "# accessing the trained models\n",
        "\n",
        "teacher_checkpoint_path = '/content/drive/MyDrive/Distillations/teacher_model.pth'\n",
        "student_distilled_checkpoint_path = '/content/drive/MyDrive/Distillations/student_distilled_model.pth'\n",
        "\n",
        "checkpoint = torch.load(teacher_checkpoint_path)\n",
        "teacher = TeacherNet(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
        "teacher.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "checkpoint = torch.load(student_distilled_checkpoint_path)\n",
        "student_distilled = StudentNet().to(device)\n",
        "student_distilled.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# feeding each model one sample to see that it works\n",
        "\n",
        "sample_data, sample_label = next(iter(test_loader))\n",
        "single_image = sample_data[0:1]  # Take first image, keep batch dimension\n",
        "true_label = sample_label[0].item()\n",
        "print(f\"True label: {true_label}\")\n",
        "\n",
        "\n",
        "# Move to device\n",
        "single_image = single_image.to(device)\n",
        "\n",
        "\n",
        "#Feed through teacher\n",
        "teacher.eval()\n",
        "with torch.no_grad():\n",
        "    teacher_probs, teacher_logits = teacher(single_image)\n",
        "    teacher_pred = teacher_probs.argmax(dim=1).item()\n",
        "    teacher_confidence = teacher_probs[0, teacher_pred].item()\n",
        "\n",
        "\n",
        "\n",
        "print(f\"\\nTeacher prediction: {teacher_pred} (confidence: {teacher_confidence:.4f})\")\n",
        "print(f\"Teacher probabilities: {teacher_probs[0].cpu().numpy()}\")\n",
        "\n",
        "student_distilled.eval()\n",
        "with torch.no_grad():\n",
        "    distilled_probs, distilled_logits = student_distilled(single_image)\n",
        "    distilled_pred = distilled_probs.argmax(dim=1).item()\n",
        "    distilled_confidence = distilled_probs[0, distilled_pred].item()\n",
        "\n",
        "print(f\"\\nStudent (distilled) prediction: {distilled_pred} (confidence: {distilled_confidence:.4f})\")\n",
        "print(f\"Distilled probabilities: {distilled_probs[0].cpu().numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8Fy07OXM_9G",
        "outputId": "15972924-c849-44b0-cc0c-001d941c4d6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 513kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.70MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 13.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True label: 7\n",
            "\n",
            "Teacher prediction: 7 (confidence: 1.0000)\n",
            "Teacher probabilities: [4.0224847e-08 8.3918567e-06 1.2510760e-05 2.7998278e-06 2.1066224e-07\n",
            " 9.7254912e-08 1.6154750e-10 9.9995697e-01 2.4853202e-07 1.8769684e-05]\n",
            "\n",
            "Student (distilled) prediction: 7 (confidence: 1.0000)\n",
            "Distilled probabilities: [5.6548139e-08 8.8660936e-06 1.6182468e-05 2.9288403e-06 1.7997323e-07\n",
            " 1.5101638e-07 2.1161005e-10 9.9995589e-01 2.0029137e-07 1.5544521e-05]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FitNets Version"
      ],
      "metadata": {
        "id": "vwS_N7WIk4UK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## architecture"
      ],
      "metadata": {
        "id": "8X5oU7tqlAng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################## 3 ########################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class FitNetStudent(nn.Module):\n",
        "    \"\"\"Thin and deep student: 4 hidden layers, ~8% of teacher params\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 4 hidden layers with fewer units (teacher has 2x1200)\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.fc2 = nn.Linear(300, 300)  # This is the guided layer (middle)\n",
        "        self.fc3 = nn.Linear(300, 300)\n",
        "        self.fc4 = nn.Linear(300, 300)\n",
        "        self.fc5 = nn.Linear(300, 10)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(x))  # Guided layer activation\n",
        "        x = F.relu(self.fc3(h))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        logits = self.fc5(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "    def forward_with_hint(self, x):\n",
        "        \"\"\"Return both output and guided layer activation\"\"\"\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        guided = F.relu(self.fc2(x))  # Guided layer\n",
        "        x = F.relu(self.fc3(guided))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        logits = self.fc5(x)\n",
        "        return logits, guided\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    \"\"\"Maps student guided layer (300) to teacher hint layer (1200)\"\"\"\n",
        "    def __init__(self, student_dim=300, teacher_dim=1200):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(student_dim, teacher_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.fc(x))\n",
        "\n",
        "\n",
        "\n",
        "def get_teacher_hint(teacher, x):\n",
        "    \"\"\"Extract teacher's first hidden layer activation (hint)\"\"\"\n",
        "    x = x.view(-1, 784)\n",
        "    hint = F.relu(teacher.fc1(x))  # First hidden layer\n",
        "    return hint\n",
        "\n",
        "\n",
        "def train_stage1_hints(student, teacher, regressor, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Stage 1: Train student (up to guided layer) + regressor to match teacher hint\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    regressor = regressor.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    # Only optimize student layers up to guided + regressor\n",
        "    optimizer = optim.Adam(list(student.parameters()) + list(regressor.parameters()), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    regressor.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, _ in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher hint (first hidden layer)\n",
        "            with torch.no_grad():\n",
        "                teacher_hint = get_teacher_hint(teacher, data)\n",
        "\n",
        "            # Get student guided layer and pass through regressor\n",
        "            _, student_guided = student.forward_with_hint(data)\n",
        "            student_prediction = regressor(student_guided)\n",
        "\n",
        "            # L2 loss between regressor output and teacher hint\n",
        "            loss = F.mse_loss(student_prediction, teacher_hint)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Stage 1 Epoch {epoch+1}/{epochs}, Hint Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def train_stage2_kd(student, teacher, train_loader, temperature=20, alpha=0.1, epochs=10, lr=0.001):\n",
        "    \"\"\"Stage 2: Standard KD training (reuse from original code)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Teacher soft targets\n",
        "            with torch.no_grad():\n",
        "                teacher_soft, _ = teacher(data, temperature=temperature)\n",
        "\n",
        "            # Student outputs\n",
        "            _, student_logits = student(data)\n",
        "\n",
        "            # Distillation loss\n",
        "            student_soft = F.log_softmax(student_logits / temperature, dim=1)\n",
        "            soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n",
        "            hard_loss = F.cross_entropy(student_logits, target)\n",
        "            loss = alpha * hard_loss + (1 - alpha) * soft_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Stage 2 Epoch {epoch+1}/{epochs}, KD Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def train_fitnet(teacher, train_loader, epochs_stage1=10, epochs_stage2=20, lr=0.001, temp=20):\n",
        "    \"\"\"Complete FitNet training: Stage 1 (hints) → Stage 2 (KD)\"\"\"\n",
        "    print(\"\\n[FitNet Stage 1] Training with hints from teacher...\")\n",
        "    student = FitNetStudent()\n",
        "    regressor = Regressor(student_dim=300, teacher_dim=1200)\n",
        "\n",
        "    student = train_stage1_hints(student, teacher, regressor, train_loader,\n",
        "                                 epochs=epochs_stage1, lr=lr)\n",
        "\n",
        "    print(\"\\n[FitNet Stage 2] Knowledge distillation...\")\n",
        "    student = train_stage2_kd(student, teacher, train_loader,\n",
        "                             temperature=temp, alpha=0.1, epochs=epochs_stage2, lr=lr)\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "3yRWteCrQJu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained teacher\n",
        "teacher_checkpoint_path = '/content/drive/MyDrive/Distillations/teacher_model.pth'\n",
        "checkpoint = torch.load(teacher_checkpoint_path)\n",
        "teacher = TeacherNet(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
        "teacher.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Train FitNet\n",
        "fitnet_student = train_fitnet(teacher, train_loader,\n",
        "                               epochs_stage1=10,\n",
        "                               epochs_stage2=20,\n",
        "                               temp=20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFvvttHtlwR1",
        "outputId": "81bc6d1f-af56-4dab-a17b-05411d96ff64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[FitNet Stage 1] Training with hints from teacher...\n",
            "Stage 1 Epoch 1/10, Hint Loss: 0.0151\n",
            "Stage 1 Epoch 2/10, Hint Loss: 0.0070\n",
            "Stage 1 Epoch 3/10, Hint Loss: 0.0064\n",
            "Stage 1 Epoch 4/10, Hint Loss: 0.0062\n",
            "Stage 1 Epoch 5/10, Hint Loss: 0.0060\n",
            "Stage 1 Epoch 6/10, Hint Loss: 0.0060\n",
            "Stage 1 Epoch 7/10, Hint Loss: 0.0059\n",
            "Stage 1 Epoch 8/10, Hint Loss: 0.0059\n",
            "Stage 1 Epoch 9/10, Hint Loss: 0.0059\n",
            "Stage 1 Epoch 10/10, Hint Loss: 0.0058\n",
            "\n",
            "[FitNet Stage 2] Knowledge distillation...\n",
            "Stage 2 Epoch 1/20, KD Loss: 4.4499\n",
            "Stage 2 Epoch 2/20, KD Loss: 1.0118\n",
            "Stage 2 Epoch 3/20, KD Loss: 0.5305\n",
            "Stage 2 Epoch 4/20, KD Loss: 0.3713\n",
            "Stage 2 Epoch 5/20, KD Loss: 0.2996\n",
            "Stage 2 Epoch 6/20, KD Loss: 0.2565\n",
            "Stage 2 Epoch 7/20, KD Loss: 0.2246\n",
            "Stage 2 Epoch 8/20, KD Loss: 0.2018\n",
            "Stage 2 Epoch 9/20, KD Loss: 0.1849\n",
            "Stage 2 Epoch 10/20, KD Loss: 0.1715\n",
            "Stage 2 Epoch 11/20, KD Loss: 0.1619\n",
            "Stage 2 Epoch 12/20, KD Loss: 0.1551\n",
            "Stage 2 Epoch 13/20, KD Loss: 0.1462\n",
            "Stage 2 Epoch 14/20, KD Loss: 0.1427\n",
            "Stage 2 Epoch 15/20, KD Loss: 0.1330\n",
            "Stage 2 Epoch 16/20, KD Loss: 0.1295\n",
            "Stage 2 Epoch 17/20, KD Loss: 0.1252\n",
            "Stage 2 Epoch 18/20, KD Loss: 0.1199\n",
            "Stage 2 Epoch 19/20, KD Loss: 0.1188\n",
            "Stage 2 Epoch 20/20, KD Loss: 0.1168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "fitnet_acc, fitnet_err = evaluate(fitnet_student, test_loader)\n",
        "print(f\"FitNet: {fitnet_acc:.2f}% accuracy ({fitnet_err} errors)\")\n",
        "\n",
        "import os\n",
        "save_dir = './saved_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save FitNet student model\n",
        "torch.save({\n",
        "    'model_state_dict': fitnet_student.state_dict(),\n",
        "}, f'{save_dir}/fitnet_student_model.pth')\n",
        "\n",
        "print(f\"✓ FitNet model saved to {save_dir}/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2kJq8mbmMiB",
        "outputId": "eb56c9e6-b437-4c68-d217-b6b3933d314c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FitNet: 98.83% accuracy (117 errors)\n",
            "✓ FitNet model saved to ./saved_models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OYcUdFaMpHyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relational KD\n"
      ],
      "metadata": {
        "id": "P01gbJKMpTId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from models import TeacherNet, StudentNet\n",
        "\n",
        "# --- 1. Define Helper Functions Locally to Avoid Import Errors ---\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy.\n",
        "    (Defined locally to ensure it works even if not in models.py)\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # Support both models that return (output, feature) and just (output)\n",
        "            out = model(data, temperature=1.0)\n",
        "            if isinstance(out, tuple):\n",
        "                outputs = out[0]\n",
        "            else:\n",
        "                outputs = out\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    errors = total - correct\n",
        "    return accuracy, errors\n",
        "\n",
        "def rkd_distance_loss(student_emb, teacher_emb):\n",
        "    \"\"\"\n",
        "    Relational Knowledge Distillation - Distance Loss\n",
        "    Forces student to mimic the pairwise distances found in the teacher's embedding space.\n",
        "    \"\"\"\n",
        "    # Compute pairwise distance matrices (batch_size x batch_size)\n",
        "    # p=2 means Euclidean distance\n",
        "    t_dist = torch.cdist(teacher_emb, teacher_emb, p=2)\n",
        "    s_dist = torch.cdist(student_emb, student_emb, p=2)\n",
        "\n",
        "    # Normalize distances by the mean of the non-zero elements\n",
        "    # (This makes the loss scale-invariant)\n",
        "    t_mean = t_dist[t_dist > 0].mean()\n",
        "    s_mean = s_dist[s_dist > 0].mean()\n",
        "\n",
        "    t_dist_norm = t_dist / t_mean\n",
        "    s_dist_norm = s_dist / s_mean\n",
        "\n",
        "    # The loss is the Huber loss (smooth L1) between the normalized distance matrices\n",
        "    loss = F.smooth_l1_loss(s_dist_norm, t_dist_norm)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_features(model, x, is_teacher=False):\n",
        "    \"\"\"\n",
        "    Manually run forward pass up to the penultimate layer to get embeddings.\n",
        "    \"\"\"\n",
        "    x = x.view(-1, 784)\n",
        "\n",
        "    if isinstance(model, TeacherNet):\n",
        "        # Teacher: fc1 -> relu -> dropout -> fc2 -> relu -> dropout -> [EMBEDDING] -> fc3\n",
        "        x = F.relu(model.fc1(x))\n",
        "        x = model.dropout(x)\n",
        "        x = F.relu(model.fc2(x))\n",
        "        # We capture the features here (after 2nd ReLU, before final dropout/classifier)\n",
        "        return x\n",
        "\n",
        "    elif isinstance(model, StudentNet):\n",
        "        # Student: fc1 -> relu -> fc2 -> relu -> [EMBEDDING] -> fc3\n",
        "        x = F.relu(model.fc1(x))\n",
        "        x = F.relu(model.fc2(x))\n",
        "        return x\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def train_student_rkd(student, teacher, train_loader, epochs=10, lr=0.001, beta=1.0):\n",
        "    \"\"\"\n",
        "    Train student using RKD (Distance) + Cross Entropy.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval() # Freeze teacher\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"Training RKD Student (Beta={beta})...\")\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_rkd_loss = 0\n",
        "        total_task_loss = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. Get Teacher Embeddings\n",
        "            with torch.no_grad():\n",
        "                teacher_emb = get_features(teacher, data, is_teacher=True)\n",
        "\n",
        "            # 2. Get Student Embeddings and Logits\n",
        "            student_emb = get_features(student, data, is_teacher=False)\n",
        "            student_logits = student.fc3(student_emb)\n",
        "\n",
        "            # 3. Calculate Losses\n",
        "            task_loss = F.cross_entropy(student_logits, target)\n",
        "            rkd_loss_val = rkd_distance_loss(student_emb, teacher_emb)\n",
        "\n",
        "            # Combined Loss\n",
        "            loss = task_loss + (beta * rkd_loss_val)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_rkd_loss += rkd_loss_val.item()\n",
        "            total_task_loss += task_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_rkd = total_rkd_loss / len(train_loader)\n",
        "        print(f\"RKD Epoch {epoch+1}/{epochs} | Total: {avg_loss:.4f} | RKD: {avg_rkd:.4f} | Task: {total_task_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "gKJDp29lwu42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. Main Execution ---\n",
        "\n",
        "def main_rkd():\n",
        "    from torchvision import datasets, transforms\n",
        "    from torch.utils.data import DataLoader\n",
        "    import os\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomAffine(degrees=0, translate=(2/28, 2/28)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Load Pre-trained Teacher\n",
        "    teacher_path = '/content/drive/MyDrive/Distillations/teacher_model.pth'\n",
        "\n",
        "    if os.path.exists(teacher_path):\n",
        "        print(f\"Loading teacher from {teacher_path}\")\n",
        "        checkpoint = torch.load(teacher_path)\n",
        "        teacher = TeacherNet(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
        "        teacher.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"Teacher model not found! Please run the first section to train/save the teacher.\")\n",
        "        return\n",
        "\n",
        "    # Initialize RKD Student (Same architecture as Normal Student: 2x800)\n",
        "    student_rkd = StudentNet().to(device)\n",
        "\n",
        "    # Train with RKD\n",
        "    student_rkd = train_student_rkd(student_rkd, teacher, train_loader, epochs=20, lr=0.001, beta=100)\n",
        "\n",
        "    # Evaluate\n",
        "    rkd_acc, rkd_err = evaluate(student_rkd, test_loader)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"RKD Student Results: {rkd_acc:.2f}% accuracy ({rkd_err} errors)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Save\n",
        "    save_dir = './saved_models'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    torch.save({\n",
        "        'model_state_dict': student_rkd.state_dict(),\n",
        "    }, f'{save_dir}/student_rkd_model.pth')\n",
        "    print(f\"✓ RKD model saved to {save_dir}/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_rkd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8RryM73wvib",
        "outputId": "e47bc787-4d2e-45e6-efdb-72f198f062b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading teacher from /content/drive/MyDrive/Distillations/teacher_model.pth\n",
            "Training RKD Student (Beta=100)...\n",
            "RKD Epoch 1/20 | Total: 0.9601 | RKD: 0.0042 | Task: 0.5376\n",
            "RKD Epoch 2/20 | Total: 0.2273 | RKD: 0.0013 | Task: 0.1019\n",
            "RKD Epoch 3/20 | Total: 0.1632 | RKD: 0.0009 | Task: 0.0712\n",
            "RKD Epoch 4/20 | Total: 0.1397 | RKD: 0.0008 | Task: 0.0616\n",
            "RKD Epoch 5/20 | Total: 0.1213 | RKD: 0.0007 | Task: 0.0517\n",
            "RKD Epoch 6/20 | Total: 0.1146 | RKD: 0.0007 | Task: 0.0486\n",
            "RKD Epoch 7/20 | Total: 0.1066 | RKD: 0.0006 | Task: 0.0446\n",
            "RKD Epoch 8/20 | Total: 0.0998 | RKD: 0.0006 | Task: 0.0407\n",
            "RKD Epoch 9/20 | Total: 0.0959 | RKD: 0.0006 | Task: 0.0390\n",
            "RKD Epoch 10/20 | Total: 0.0924 | RKD: 0.0005 | Task: 0.0382\n",
            "RKD Epoch 11/20 | Total: 0.0874 | RKD: 0.0005 | Task: 0.0357\n",
            "RKD Epoch 12/20 | Total: 0.0838 | RKD: 0.0005 | Task: 0.0344\n",
            "RKD Epoch 13/20 | Total: 0.0797 | RKD: 0.0005 | Task: 0.0318\n",
            "RKD Epoch 14/20 | Total: 0.0777 | RKD: 0.0005 | Task: 0.0303\n",
            "RKD Epoch 15/20 | Total: 0.0798 | RKD: 0.0005 | Task: 0.0319\n",
            "RKD Epoch 16/20 | Total: 0.0772 | RKD: 0.0005 | Task: 0.0309\n",
            "RKD Epoch 17/20 | Total: 0.0735 | RKD: 0.0004 | Task: 0.0288\n",
            "RKD Epoch 18/20 | Total: 0.0745 | RKD: 0.0004 | Task: 0.0305\n",
            "RKD Epoch 19/20 | Total: 0.0686 | RKD: 0.0004 | Task: 0.0265\n",
            "RKD Epoch 20/20 | Total: 0.0701 | RKD: 0.0004 | Task: 0.0283\n",
            "\n",
            "============================================================\n",
            "RKD Student Results: 98.82% accuracy (118 errors)\n",
            "============================================================\n",
            "✓ RKD model saved to ./saved_models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "upssdCVfxWfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm Change\n"
      ],
      "metadata": {
        "id": "iNuTP-bHkMOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla"
      ],
      "metadata": {
        "id": "wPsZ4RpCkReK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile models.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TeacherNet(nn.Module):\n",
        "    \"\"\"Large teacher network: 2 hidden layers of 1200 ReLU units\"\"\"\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 1200)\n",
        "        self.fc2 = nn.Linear(1200, 1200)\n",
        "        self.fc3 = nn.Linear(1200, 10)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc3(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "\n",
        "class StudentNet(nn.Module):\n",
        "    \"\"\"Small student network: 2 hidden layers of 800 ReLU units\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 800)\n",
        "        self.bn1 = nn.BatchNorm1d(800)      # NEW: BatchNorm after first linear\n",
        "        self.fc2 = nn.Linear(800, 800)\n",
        "        self.bn2 = nn.BatchNorm1d(800)      # NEW: BatchNorm after second linear\n",
        "        self.fc3 = nn.Linear(800, 10)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))   # fc1 → bn1 → relu\n",
        "        identity = x                         # NEW: Save for residual\n",
        "        x = F.relu(self.bn2(self.fc2(x))) + identity  # fc2 → bn2 → relu → ADD identity\n",
        "        logits = self.fc3(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "\n",
        "def distillation_loss(student_logits, teacher_soft_targets, hard_targets,\n",
        "                      temperature, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Combined loss for distillation.\n",
        "\n",
        "    Args:\n",
        "        student_logits: Raw outputs from student model\n",
        "        teacher_soft_targets: Soft probabilities from teacher (at temperature T)\n",
        "        hard_targets: Ground truth labels\n",
        "        temperature: Temperature for distillation\n",
        "        alpha: Weight for hard target loss (1-alpha is weight for soft targets)\n",
        "\n",
        "    Returns:\n",
        "        Combined loss\n",
        "    \"\"\"\n",
        "    # Soft target loss: KL divergence between student and teacher (both at temperature T)\n",
        "    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n",
        "    soft_loss = F.kl_div(student_soft, teacher_soft_targets, reduction='batchmean')\n",
        "\n",
        "    # Scale by T^2 as per paper (gradients scale as 1/T^2)\n",
        "    soft_loss = soft_loss * (temperature ** 2)\n",
        "\n",
        "    # Hard target loss: Standard cross-entropy (at temperature 1)\n",
        "    hard_loss = F.cross_entropy(student_logits, hard_targets)\n",
        "\n",
        "    # Weighted combination\n",
        "    return alpha * hard_loss + (1 - alpha) * soft_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUwymy_YlR_p",
        "outputId": "bca0885a-1431-4065-a7bd-255311961d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing models.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################## 2 ########################\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from models import TeacherNet, StudentNet, distillation_loss\n",
        "\n",
        "\n",
        "def train_teacher(model, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train the large teacher model with dropout regularization\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, logits = model(data)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Teacher Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_student_normal(model, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train student model normally (baseline)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, logits = model(data)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Student (normal) Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_student_distilled(student, teacher, train_loader, temperature=20,\n",
        "                           alpha=0.1, epochs=10, lr=0.001):\n",
        "    \"\"\"Train student model using knowledge distillation\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()  # Teacher is frozen\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher's soft targets at temperature T\n",
        "            with torch.no_grad():\n",
        "                teacher_soft_targets, _ = teacher(data, temperature=temperature)\n",
        "\n",
        "            # Get student outputs\n",
        "            _, student_logits = student(data)\n",
        "\n",
        "            # Compute distillation loss\n",
        "            loss = distillation_loss(student_logits, teacher_soft_targets,\n",
        "                                    target, temperature, alpha)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Student (distilled T={temperature}) Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    \"\"\"Evaluate model accuracy\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs, _ = model(data, temperature=1.0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    errors = total - correct\n",
        "    return accuracy, errors\n"
      ],
      "metadata": {
        "id": "8rcEj_6pl2mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "    # Rest of your code...\n",
        "\n",
        "    dropout = 0.2\n",
        "    epochs = 20\n",
        "    temp = 20\n",
        "    lr = 0.001\n",
        "    batchsize = 512\n",
        "\n",
        "\n",
        "    # Data loading with jittering (up to 2 pixels in any direction)\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomAffine(degrees=0, translate=(2/28, 2/28)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True,\n",
        "                         num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"MNIST Knowledge Distillation Experiment (Section 3)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "    # -------------------------------------------------------------------------------- #\n",
        "\n",
        "    # load the pretrained teacher model intead of retraining the teacher\n",
        "    teacher_checkpoint_path = '/content/drive/MyDrive/Distillations/teacher_model.pth'\n",
        "    checkpoint = torch.load(teacher_checkpoint_path)\n",
        "    teacher = TeacherNet(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
        "    teacher.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # -------------------------------------------------------------------------------- #\n",
        "\n",
        "    # 2. Train small student network normally (800-800 units, no regularization)\n",
        "    print(\"\\n[2/4] Training Student Network Normally (2x800 ReLU units, no regularization)...\")\n",
        "    student_normal = StudentNet()\n",
        "    student_normal = train_student_normal(student_normal, train_loader, epochs=epochs, lr=lr)\n",
        "    student_normal_acc, student_normal_err = evaluate(student_normal, test_loader)\n",
        "    print(f\"✓ Student (normal): {student_normal_acc:.2f}% accuracy ({student_normal_err} errors)\")\n",
        "\n",
        "    # 3. Train small student network with distillation (T=20)\n",
        "    print(\"\\n[3/4] Training Student Network with Distillation (T=20, alpha=0.1)...\")\n",
        "    student_distilled = StudentNet()\n",
        "    student_distilled = train_student_distilled(student_distilled, teacher, train_loader,\n",
        "                                                temperature=temp, alpha=0.1, epochs=epochs, lr=lr)\n",
        "    student_distilled_acc, student_distilled_err = evaluate(student_distilled, test_loader)\n",
        "    print(f\"✓ Student (distilled): {student_distilled_acc:.2f}% accuracy ({student_distilled_err} errors)\")\n",
        "\n",
        "\n",
        "    # 6. save models architecture and weights\n",
        "\n",
        "    # Save models\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"SAVING MODELS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    import os\n",
        "    save_dir = './saved_models'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': student_normal.state_dict(),\n",
        "    }, f'{save_dir}/student_normal_model_after_algo_change.pth')\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': student_distilled.state_dict(),\n",
        "    }, f'{save_dir}/student_distilled_model_after_algo_change.pth')\n",
        "\n",
        "    print(f\"✓ Models saved to {save_dir}/\")\n",
        "\n",
        "    # Download in Colab\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        import shutil\n",
        "\n",
        "        # Create zip file\n",
        "        shutil.make_archive('mnist_distillation_models', 'zip', save_dir)\n",
        "        files.download('mnist_distillation_models.zip')\n",
        "        print(\"✓ Models downloaded as mnist_distillation_models.zip!\")\n",
        "    except:\n",
        "        print(\"Not running in Colab - models saved locally only\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t6DnEsxamrSW",
        "outputId": "f2c92532-0488-4a1c-8f1a-e5ed31aaffd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "GPU Memory: 42.47 GB\n",
            "============================================================\n",
            "MNIST Knowledge Distillation Experiment (Section 3)\n",
            "============================================================\n",
            "\n",
            "[2/4] Training Student Network Normally (2x800 ReLU units, no regularization)...\n",
            "Student (normal) Epoch 1/20, Loss: 0.2686\n",
            "Student (normal) Epoch 2/20, Loss: 0.1262\n",
            "Student (normal) Epoch 3/20, Loss: 0.0982\n",
            "Student (normal) Epoch 4/20, Loss: 0.0867\n",
            "Student (normal) Epoch 5/20, Loss: 0.0739\n",
            "Student (normal) Epoch 6/20, Loss: 0.0673\n",
            "Student (normal) Epoch 7/20, Loss: 0.0624\n",
            "Student (normal) Epoch 8/20, Loss: 0.0589\n",
            "Student (normal) Epoch 9/20, Loss: 0.0549\n",
            "Student (normal) Epoch 10/20, Loss: 0.0507\n",
            "Student (normal) Epoch 11/20, Loss: 0.0485\n",
            "Student (normal) Epoch 12/20, Loss: 0.0444\n",
            "Student (normal) Epoch 13/20, Loss: 0.0449\n",
            "Student (normal) Epoch 14/20, Loss: 0.0406\n",
            "Student (normal) Epoch 15/20, Loss: 0.0414\n",
            "Student (normal) Epoch 16/20, Loss: 0.0389\n",
            "Student (normal) Epoch 17/20, Loss: 0.0375\n",
            "Student (normal) Epoch 18/20, Loss: 0.0341\n",
            "Student (normal) Epoch 19/20, Loss: 0.0355\n",
            "Student (normal) Epoch 20/20, Loss: 0.0346\n",
            "✓ Student (normal): 98.73% accuracy (127 errors)\n",
            "\n",
            "[3/4] Training Student Network with Distillation (T=20, alpha=0.1)...\n",
            "Student (distilled T=20) Epoch 1/20, Loss: 1.4291\n",
            "Student (distilled T=20) Epoch 2/20, Loss: 0.3974\n",
            "Student (distilled T=20) Epoch 3/20, Loss: 0.3208\n",
            "Student (distilled T=20) Epoch 4/20, Loss: 0.2855\n",
            "Student (distilled T=20) Epoch 5/20, Loss: 0.2531\n",
            "Student (distilled T=20) Epoch 6/20, Loss: 0.2564\n",
            "Student (distilled T=20) Epoch 7/20, Loss: 0.2351\n",
            "Student (distilled T=20) Epoch 8/20, Loss: 0.2261\n",
            "Student (distilled T=20) Epoch 9/20, Loss: 0.2001\n",
            "Student (distilled T=20) Epoch 10/20, Loss: 0.2179\n",
            "Student (distilled T=20) Epoch 11/20, Loss: 0.2048\n",
            "Student (distilled T=20) Epoch 12/20, Loss: 0.1999\n",
            "Student (distilled T=20) Epoch 13/20, Loss: 0.1946\n",
            "Student (distilled T=20) Epoch 14/20, Loss: 0.1883\n",
            "Student (distilled T=20) Epoch 15/20, Loss: 0.1937\n",
            "Student (distilled T=20) Epoch 16/20, Loss: 0.1850\n",
            "Student (distilled T=20) Epoch 17/20, Loss: 0.1862\n",
            "Student (distilled T=20) Epoch 18/20, Loss: 0.1821\n",
            "Student (distilled T=20) Epoch 19/20, Loss: 0.1755\n",
            "Student (distilled T=20) Epoch 20/20, Loss: 0.1712\n",
            "✓ Student (distilled): 98.92% accuracy (108 errors)\n",
            "\n",
            "============================================================\n",
            "SAVING MODELS\n",
            "============================================================\n",
            "✓ Models saved to ./saved_models/\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5661553d-3a71-4c22-bf60-665e0cae482e\", \"mnist_distillation_models.zip\", 9532524)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Models downloaded as mnist_distillation_models.zip!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FitNets"
      ],
      "metadata": {
        "id": "oLKj6NSGntxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################## 3 ########################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class FitNetStudent(nn.Module):\n",
        "    \"\"\"Thin and deep student: 4 hidden layers with ResNet + BatchNorm\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.bn1 = nn.BatchNorm1d(300)\n",
        "        self.fc2 = nn.Linear(300, 300)\n",
        "        self.bn2 = nn.BatchNorm1d(300)\n",
        "        self.fc3 = nn.Linear(300, 300)\n",
        "        self.bn3 = nn.BatchNorm1d(300)\n",
        "        self.fc4 = nn.Linear(300, 300)\n",
        "        self.bn4 = nn.BatchNorm1d(300)\n",
        "        self.fc5 = nn.Linear(300, 10)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        identity = x\n",
        "        x = F.relu(self.bn2(self.fc2(x))) + identity\n",
        "\n",
        "        identity = x\n",
        "        x = F.relu(self.bn3(self.fc3(x))) + identity\n",
        "\n",
        "        identity = x\n",
        "        x = F.relu(self.bn4(self.fc4(x))) + identity\n",
        "\n",
        "        logits = self.fc5(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "    def forward_with_hint(self, x):\n",
        "        \"\"\"Return both output and guided layer activation\"\"\"\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        identity = x\n",
        "        guided = F.relu(self.bn2(self.fc2(x))) + identity\n",
        "\n",
        "        identity = guided\n",
        "        x = F.relu(self.bn3(self.fc3(guided))) + identity\n",
        "\n",
        "        identity = x\n",
        "        x = F.relu(self.bn4(self.fc4(x))) + identity\n",
        "\n",
        "        logits = self.fc5(x)\n",
        "        return logits, guided\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    \"\"\"Maps student guided layer (300) to teacher hint layer (1200)\"\"\"\n",
        "    def __init__(self, student_dim=300, teacher_dim=1200):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(student_dim, teacher_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.fc(x))\n",
        "\n",
        "\n",
        "\n",
        "def get_teacher_hint(teacher, x):\n",
        "    \"\"\"Extract teacher's first hidden layer activation (hint)\"\"\"\n",
        "    x = x.view(-1, 784)\n",
        "    hint = F.relu(teacher.fc1(x))  # First hidden layer\n",
        "    return hint\n",
        "\n",
        "\n",
        "def train_stage1_hints(student, teacher, regressor, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Stage 1: Train student (up to guided layer) + regressor to match teacher hint\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    regressor = regressor.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    # Only optimize student layers up to guided + regressor\n",
        "    optimizer = optim.Adam(list(student.parameters()) + list(regressor.parameters()), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    regressor.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, _ in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher hint (first hidden layer)\n",
        "            with torch.no_grad():\n",
        "                teacher_hint = get_teacher_hint(teacher, data)\n",
        "\n",
        "            # Get student guided layer and pass through regressor\n",
        "            _, student_guided = student.forward_with_hint(data)\n",
        "            student_prediction = regressor(student_guided)\n",
        "\n",
        "            # L2 loss between regressor output and teacher hint\n",
        "            loss = F.mse_loss(student_prediction, teacher_hint)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Stage 1 Epoch {epoch+1}/{epochs}, Hint Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def train_stage2_kd(student, teacher, train_loader, temperature=20, alpha=0.1, epochs=10, lr=0.001):\n",
        "    \"\"\"Stage 2: Standard KD training (reuse from original code)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Teacher soft targets\n",
        "            with torch.no_grad():\n",
        "                teacher_soft, _ = teacher(data, temperature=temperature)\n",
        "\n",
        "            # Student outputs\n",
        "            _, student_logits = student(data)\n",
        "\n",
        "            # Distillation loss\n",
        "            student_soft = F.log_softmax(student_logits / temperature, dim=1)\n",
        "            soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n",
        "            hard_loss = F.cross_entropy(student_logits, target)\n",
        "            loss = alpha * hard_loss + (1 - alpha) * soft_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Stage 2 Epoch {epoch+1}/{epochs}, KD Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def train_fitnet(teacher, train_loader, epochs_stage1=10, epochs_stage2=20, lr=0.001, temp=20):\n",
        "    \"\"\"Complete FitNet training: Stage 1 (hints) → Stage 2 (KD)\"\"\"\n",
        "    print(\"\\n[FitNet Stage 1] Training with hints from teacher...\")\n",
        "    student = FitNetStudent()\n",
        "    regressor = Regressor(student_dim=300, teacher_dim=1200)\n",
        "\n",
        "    student = train_stage1_hints(student, teacher, regressor, train_loader,\n",
        "                                 epochs=epochs_stage1, lr=lr)\n",
        "\n",
        "    print(\"\\n[FitNet Stage 2] Knowledge distillation...\")\n",
        "    student = train_stage2_kd(student, teacher, train_loader,\n",
        "                             temperature=temp, alpha=0.1, epochs=epochs_stage2, lr=lr)\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "qt78X90Kmz_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Data loading with jittering (up to 2 pixels in any direction)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=0, translate=(2/28, 2/28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "\n",
        "# Load trained teacher\n",
        "teacher_checkpoint_path = '/content/drive/MyDrive/Distillations/teacher_model.pth'\n",
        "checkpoint = torch.load(teacher_checkpoint_path)\n",
        "teacher = TeacherNet(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
        "teacher.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Train FitNet\n",
        "fitnet_student = train_fitnet(teacher, train_loader,\n",
        "                               epochs_stage1=10,\n",
        "                               epochs_stage2=20,\n",
        "                               temp=20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM8okcgPtaVW",
        "outputId": "c8024628-195f-411b-b920-86f0480abf90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[FitNet Stage 1] Training with hints from teacher...\n",
            "Stage 1 Epoch 1/10, Hint Loss: 0.0353\n",
            "Stage 1 Epoch 2/10, Hint Loss: 0.0297\n",
            "Stage 1 Epoch 3/10, Hint Loss: 0.0289\n",
            "Stage 1 Epoch 4/10, Hint Loss: 0.0284\n",
            "Stage 1 Epoch 5/10, Hint Loss: 0.0280\n",
            "Stage 1 Epoch 6/10, Hint Loss: 0.0275\n",
            "Stage 1 Epoch 7/10, Hint Loss: 0.0270\n",
            "Stage 1 Epoch 8/10, Hint Loss: 0.0267\n",
            "Stage 1 Epoch 9/10, Hint Loss: 0.0264\n",
            "Stage 1 Epoch 10/10, Hint Loss: 0.0262\n",
            "\n",
            "[FitNet Stage 2] Knowledge distillation...\n",
            "Stage 2 Epoch 1/20, KD Loss: 1.4957\n",
            "Stage 2 Epoch 2/20, KD Loss: 0.4002\n",
            "Stage 2 Epoch 3/20, KD Loss: 0.3248\n",
            "Stage 2 Epoch 4/20, KD Loss: 0.2756\n",
            "Stage 2 Epoch 5/20, KD Loss: 0.2670\n",
            "Stage 2 Epoch 6/20, KD Loss: 0.2626\n",
            "Stage 2 Epoch 7/20, KD Loss: 0.2448\n",
            "Stage 2 Epoch 8/20, KD Loss: 0.2228\n",
            "Stage 2 Epoch 9/20, KD Loss: 0.2269\n",
            "Stage 2 Epoch 10/20, KD Loss: 0.2136\n",
            "Stage 2 Epoch 11/20, KD Loss: 0.2256\n",
            "Stage 2 Epoch 12/20, KD Loss: 0.2053\n",
            "Stage 2 Epoch 13/20, KD Loss: 0.2008\n",
            "Stage 2 Epoch 14/20, KD Loss: 0.1945\n",
            "Stage 2 Epoch 15/20, KD Loss: 0.2009\n",
            "Stage 2 Epoch 16/20, KD Loss: 0.1972\n",
            "Stage 2 Epoch 17/20, KD Loss: 0.1967\n",
            "Stage 2 Epoch 18/20, KD Loss: 0.2045\n",
            "Stage 2 Epoch 19/20, KD Loss: 0.1928\n",
            "Stage 2 Epoch 20/20, KD Loss: 0.1982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save\n",
        "import os\n",
        "save_dir = './saved_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save FitNet student model\n",
        "torch.save({\n",
        "    'model_state_dict': fitnet_student.state_dict(),\n",
        "}, f'{save_dir}/fitnet_student_model_after_algo_change.pth')\n",
        "\n",
        "print(f\"✓ FitNet model saved to {save_dir}/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPAr5rQ_t0m_",
        "outputId": "65ec8962-8b5c-4a8a-8ad8-ba08ad80f1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ FitNet model saved to ./saved_models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Relational KD"
      ],
      "metadata": {
        "id": "gV4dd4jDveRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from models import TeacherNet, StudentNet\n",
        "\n",
        "# --- 1. Define Helper Functions Locally to Avoid Import Errors ---\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy.\n",
        "    (Defined locally to ensure it works even if not in models.py)\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # Support both models that return (output, feature) and just (output)\n",
        "            out = model(data, temperature=1.0)\n",
        "            if isinstance(out, tuple):\n",
        "                outputs = out[0]\n",
        "            else:\n",
        "                outputs = out\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    errors = total - correct\n",
        "    return accuracy, errors\n",
        "\n",
        "def rkd_distance_loss(student_emb, teacher_emb):\n",
        "    \"\"\"\n",
        "    Relational Knowledge Distillation - Distance Loss\n",
        "    Forces student to mimic the pairwise distances found in the teacher's embedding space.\n",
        "    \"\"\"\n",
        "    # Compute pairwise distance matrices (batch_size x batch_size)\n",
        "    # p=2 means Euclidean distance\n",
        "    t_dist = torch.cdist(teacher_emb, teacher_emb, p=2)\n",
        "    s_dist = torch.cdist(student_emb, student_emb, p=2)\n",
        "\n",
        "    # Normalize distances by the mean of the non-zero elements\n",
        "    # (This makes the loss scale-invariant)\n",
        "    t_mean = t_dist[t_dist > 0].mean()\n",
        "    s_mean = s_dist[s_dist > 0].mean()\n",
        "\n",
        "    t_dist_norm = t_dist / t_mean\n",
        "    s_dist_norm = s_dist / s_mean\n",
        "\n",
        "    # The loss is the Huber loss (smooth L1) between the normalized distance matrices\n",
        "    loss = F.smooth_l1_loss(s_dist_norm, t_dist_norm)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_features(model, x, is_teacher=False):\n",
        "    x = x.view(-1, 784)\n",
        "\n",
        "    if isinstance(model, TeacherNet):\n",
        "        x = F.relu(model.fc1(x))\n",
        "        x = model.dropout(x)\n",
        "        x = F.relu(model.fc2(x))\n",
        "        return x                        # Teacher unchanged\n",
        "\n",
        "    elif isinstance(model, StudentNet):\n",
        "        x = F.relu(model.bn1(model.fc1(x)))      # fc1 → bn1 → relu\n",
        "        identity = x                              # Save for residual\n",
        "        x = F.relu(model.bn2(model.fc2(x))) + identity  # fc2 → bn2 → relu + skip\n",
        "        return x                                  # Return pre-fc3 embeddings\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def train_student_rkd(student, teacher, train_loader, epochs=10, lr=0.001, beta=1.0):\n",
        "    \"\"\"\n",
        "    Train student using RKD (Distance) + Cross Entropy.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval() # Freeze teacher\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"Training RKD Student (Beta={beta})...\")\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_rkd_loss = 0\n",
        "        total_task_loss = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. Get Teacher Embeddings\n",
        "            with torch.no_grad():\n",
        "                teacher_emb = get_features(teacher, data, is_teacher=True)\n",
        "\n",
        "            # 2. Get Student Embeddings and Logits\n",
        "            student_emb = get_features(student, data, is_teacher=False)\n",
        "            student_logits = student.fc3(student_emb)\n",
        "\n",
        "            # 3. Calculate Losses\n",
        "            task_loss = F.cross_entropy(student_logits, target)\n",
        "            rkd_loss_val = rkd_distance_loss(student_emb, teacher_emb)\n",
        "\n",
        "            # Combined Loss\n",
        "            loss = task_loss + (beta * rkd_loss_val)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_rkd_loss += rkd_loss_val.item()\n",
        "            total_task_loss += task_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_rkd = total_rkd_loss / len(train_loader)\n",
        "        print(f\"RKD Epoch {epoch+1}/{epochs} | Total: {avg_loss:.4f} | RKD: {avg_rkd:.4f} | Task: {total_task_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "Dw3nZr-DvfH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. Main Execution ---\n",
        "\n",
        "def main_rkd():\n",
        "    from torchvision import datasets, transforms\n",
        "    from torch.utils.data import DataLoader\n",
        "    import os\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomAffine(degrees=0, translate=(2/28, 2/28)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.MNIST('./data', train=False, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    # Load Pre-trained Teacher\n",
        "    teacher_path = '/content/drive/MyDrive/Distillations/teacher_model.pth'\n",
        "\n",
        "    if os.path.exists(teacher_path):\n",
        "        print(f\"Loading teacher from {teacher_path}\")\n",
        "        checkpoint = torch.load(teacher_path)\n",
        "        teacher = TeacherNet(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
        "        teacher.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"Teacher model not found! Please run the first section to train/save the teacher.\")\n",
        "        return\n",
        "\n",
        "    # Initialize RKD Student (Same architecture as Normal Student: 2x800)\n",
        "    student_rkd = StudentNet().to(device)\n",
        "\n",
        "    # Train with RKD\n",
        "    student_rkd = train_student_rkd(student_rkd, teacher, train_loader, epochs=20, lr=0.001, beta=100)\n",
        "\n",
        "    # Evaluate\n",
        "    rkd_acc, rkd_err = evaluate(student_rkd, test_loader)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"RKD Student Results: {rkd_acc:.2f}% accuracy ({rkd_err} errors)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Save\n",
        "    save_dir = './saved_models'\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    torch.save({\n",
        "        'model_state_dict': student_rkd.state_dict(),\n",
        "    }, f'{save_dir}/student_rkd_model_after_algo_change.pth')\n",
        "    print(f\"✓ RKD model saved to {save_dir}/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_rkd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQpjUEdbv_pM",
        "outputId": "25a727d9-3d99-456f-fa62-244b0ec48473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading teacher from /content/drive/MyDrive/Distillations/teacher_model.pth\n",
            "Training RKD Student (Beta=100)...\n",
            "RKD Epoch 1/20 | Total: 0.7007 | RKD: 0.0044 | Task: 0.2559\n",
            "RKD Epoch 2/20 | Total: 0.2843 | RKD: 0.0019 | Task: 0.0986\n",
            "RKD Epoch 3/20 | Total: 0.2317 | RKD: 0.0015 | Task: 0.0791\n",
            "RKD Epoch 4/20 | Total: 0.2008 | RKD: 0.0013 | Task: 0.0669\n",
            "RKD Epoch 5/20 | Total: 0.1789 | RKD: 0.0012 | Task: 0.0594\n",
            "RKD Epoch 6/20 | Total: 0.1674 | RKD: 0.0011 | Task: 0.0549\n",
            "RKD Epoch 7/20 | Total: 0.1563 | RKD: 0.0011 | Task: 0.0505\n",
            "RKD Epoch 8/20 | Total: 0.1522 | RKD: 0.0010 | Task: 0.0490\n",
            "RKD Epoch 9/20 | Total: 0.1380 | RKD: 0.0009 | Task: 0.0436\n",
            "RKD Epoch 10/20 | Total: 0.1378 | RKD: 0.0009 | Task: 0.0438\n",
            "RKD Epoch 11/20 | Total: 0.1299 | RKD: 0.0009 | Task: 0.0402\n",
            "RKD Epoch 12/20 | Total: 0.1233 | RKD: 0.0008 | Task: 0.0394\n",
            "RKD Epoch 13/20 | Total: 0.1208 | RKD: 0.0008 | Task: 0.0381\n",
            "RKD Epoch 14/20 | Total: 0.1203 | RKD: 0.0008 | Task: 0.0370\n",
            "RKD Epoch 15/20 | Total: 0.1151 | RKD: 0.0008 | Task: 0.0348\n",
            "RKD Epoch 16/20 | Total: 0.1139 | RKD: 0.0008 | Task: 0.0340\n",
            "RKD Epoch 17/20 | Total: 0.1119 | RKD: 0.0008 | Task: 0.0331\n",
            "RKD Epoch 18/20 | Total: 0.1075 | RKD: 0.0008 | Task: 0.0311\n",
            "RKD Epoch 19/20 | Total: 0.1045 | RKD: 0.0007 | Task: 0.0302\n",
            "RKD Epoch 20/20 | Total: 0.1063 | RKD: 0.0008 | Task: 0.0307\n",
            "\n",
            "============================================================\n",
            "RKD Student Results: 99.08% accuracy (92 errors)\n",
            "============================================================\n",
            "✓ RKD model saved to ./saved_models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# McNemar Test - before and after algo change to check if significant"
      ],
      "metadata": {
        "id": "bRySP2DDWkCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load architectures before"
      ],
      "metadata": {
        "id": "Xx6owmuGcHBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################## 1 ########################\n",
        "\n",
        "%%writefile modelsbefore.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TeacherNet(nn.Module):\n",
        "    \"\"\"Large teacher network: 2 hidden layers of 1200 ReLU units\"\"\"\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 1200)\n",
        "        self.fc2 = nn.Linear(1200, 1200)\n",
        "        self.fc3 = nn.Linear(1200, 10)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc3(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "\n",
        "class StudentNetbefore(nn.Module):\n",
        "    \"\"\"Small student network: 2 hidden layers of 800 ReLU units\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 800)\n",
        "        self.fc2 = nn.Linear(800, 800)\n",
        "        self.fc3 = nn.Linear(800, 10)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "\n",
        "def distillation_loss(student_logits, teacher_soft_targets, hard_targets,\n",
        "                      temperature, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Combined loss for distillation.\n",
        "\n",
        "    Args:\n",
        "        student_logits: Raw outputs from student model\n",
        "        teacher_soft_targets: Soft probabilities from teacher (at temperature T)\n",
        "        hard_targets: Ground truth labels\n",
        "        temperature: Temperature for distillation\n",
        "        alpha: Weight for hard target loss (1-alpha is weight for soft targets)\n",
        "\n",
        "    Returns:\n",
        "        Combined loss\n",
        "    \"\"\"\n",
        "    # Soft target loss: KL divergence between student and teacher (both at temperature T)\n",
        "    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n",
        "    soft_loss = F.kl_div(student_soft, teacher_soft_targets, reduction='batchmean')\n",
        "\n",
        "    # Scale by T^2 as per paper (gradients scale as 1/T^2)\n",
        "    soft_loss = soft_loss * (temperature ** 2)\n",
        "\n",
        "    # Hard target loss: Standard cross-entropy (at temperature 1)\n",
        "    hard_loss = F.cross_entropy(student_logits, hard_targets)\n",
        "\n",
        "    # Weighted combination\n",
        "    return alpha * hard_loss + (1 - alpha) * soft_loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuhU_rcacN09",
        "outputId": "51a8c0db-0c77-4325-df96-0d53907f2d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modelsbefore.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################## 2 ########################\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from modelsbefore import TeacherNet, StudentNetbefore, distillation_loss\n",
        "\n",
        "\n",
        "def train_teacher(model, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train the large teacher model with dropout regularization\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, logits = model(data)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Teacher Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_student_normal(model, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train student model normally (baseline)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, logits = model(data)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Student (normal) Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_student_distilled(student, teacher, train_loader, temperature=20,\n",
        "                           alpha=0.1, epochs=10, lr=0.001):\n",
        "    \"\"\"Train student model using knowledge distillation\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()  # Teacher is frozen\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher's soft targets at temperature T\n",
        "            with torch.no_grad():\n",
        "                teacher_soft_targets, _ = teacher(data, temperature=temperature)\n",
        "\n",
        "            # Get student outputs\n",
        "            _, student_logits = student(data)\n",
        "\n",
        "            # Compute distillation loss\n",
        "            loss = distillation_loss(student_logits, teacher_soft_targets,\n",
        "                                    target, temperature, alpha)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Student (distilled T={temperature}) Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    \"\"\"Evaluate model accuracy\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs, _ = model(data, temperature=1.0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    errors = total - correct\n",
        "    return accuracy, errors\n"
      ],
      "metadata": {
        "id": "1wZlnn41cN3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################## 3 ########################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class FitNetStudentbefore(nn.Module):\n",
        "    \"\"\"Thin and deep student: 4 hidden layers, ~8% of teacher params\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 4 hidden layers with fewer units (teacher has 2x1200)\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.fc2 = nn.Linear(300, 300)  # This is the guided layer (middle)\n",
        "        self.fc3 = nn.Linear(300, 300)\n",
        "        self.fc4 = nn.Linear(300, 300)\n",
        "        self.fc5 = nn.Linear(300, 10)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(x))  # Guided layer activation\n",
        "        x = F.relu(self.fc3(h))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        logits = self.fc5(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "    def forward_with_hint(self, x):\n",
        "        \"\"\"Return both output and guided layer activation\"\"\"\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        guided = F.relu(self.fc2(x))  # Guided layer\n",
        "        x = F.relu(self.fc3(guided))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        logits = self.fc5(x)\n",
        "        return logits, guided\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    \"\"\"Maps student guided layer (300) to teacher hint layer (1200)\"\"\"\n",
        "    def __init__(self, student_dim=300, teacher_dim=1200):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(student_dim, teacher_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.fc(x))\n",
        "\n",
        "\n",
        "\n",
        "def get_teacher_hint(teacher, x):\n",
        "    \"\"\"Extract teacher's first hidden layer activation (hint)\"\"\"\n",
        "    x = x.view(-1, 784)\n",
        "    hint = F.relu(teacher.fc1(x))  # First hidden layer\n",
        "    return hint\n",
        "\n",
        "\n",
        "def train_stage1_hints(student, teacher, regressor, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Stage 1: Train student (up to guided layer) + regressor to match teacher hint\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    regressor = regressor.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    # Only optimize student layers up to guided + regressor\n",
        "    optimizer = optim.Adam(list(student.parameters()) + list(regressor.parameters()), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    regressor.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, _ in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher hint (first hidden layer)\n",
        "            with torch.no_grad():\n",
        "                teacher_hint = get_teacher_hint(teacher, data)\n",
        "\n",
        "            # Get student guided layer and pass through regressor\n",
        "            _, student_guided = student.forward_with_hint(data)\n",
        "            student_prediction = regressor(student_guided)\n",
        "\n",
        "            # L2 loss between regressor output and teacher hint\n",
        "            loss = F.mse_loss(student_prediction, teacher_hint)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Stage 1 Epoch {epoch+1}/{epochs}, Hint Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def train_stage2_kd(student, teacher, train_loader, temperature=20, alpha=0.1, epochs=10, lr=0.001):\n",
        "    \"\"\"Stage 2: Standard KD training (reuse from original code)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Teacher soft targets\n",
        "            with torch.no_grad():\n",
        "                teacher_soft, _ = teacher(data, temperature=temperature)\n",
        "\n",
        "            # Student outputs\n",
        "            _, student_logits = student(data)\n",
        "\n",
        "            # Distillation loss\n",
        "            student_soft = F.log_softmax(student_logits / temperature, dim=1)\n",
        "            soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n",
        "            hard_loss = F.cross_entropy(student_logits, target)\n",
        "            loss = alpha * hard_loss + (1 - alpha) * soft_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Stage 2 Epoch {epoch+1}/{epochs}, KD Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def train_fitnet(teacher, train_loader, epochs_stage1=10, epochs_stage2=20, lr=0.001, temp=20):\n",
        "    \"\"\"Complete FitNet training: Stage 1 (hints) → Stage 2 (KD)\"\"\"\n",
        "    print(\"\\n[FitNet Stage 1] Training with hints from teacher...\")\n",
        "    student = FitNetStudentbefore()\n",
        "    regressor = Regressor(student_dim=300, teacher_dim=1200)\n",
        "\n",
        "    student = train_stage1_hints(student, teacher, regressor, train_loader,\n",
        "                                 epochs=epochs_stage1, lr=lr)\n",
        "\n",
        "    print(\"\\n[FitNet Stage 2] Knowledge distillation...\")\n",
        "    student = train_stage2_kd(student, teacher, train_loader,\n",
        "                             temperature=temp, alpha=0.1, epochs=epochs_stage2, lr=lr)\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "z-_vSpq0cN5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from modelsbefore import TeacherNet, StudentNetbefore\n",
        "\n",
        "# --- 1. Define Helper Functions Locally to Avoid Import Errors ---\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy.\n",
        "    (Defined locally to ensure it works even if not in models.py)\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # Support both models that return (output, feature) and just (output)\n",
        "            out = model(data, temperature=1.0)\n",
        "            if isinstance(out, tuple):\n",
        "                outputs = out[0]\n",
        "            else:\n",
        "                outputs = out\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    errors = total - correct\n",
        "    return accuracy, errors\n",
        "\n",
        "def rkd_distance_loss(student_emb, teacher_emb):\n",
        "    \"\"\"\n",
        "    Relational Knowledge Distillation - Distance Loss\n",
        "    Forces student to mimic the pairwise distances found in the teacher's embedding space.\n",
        "    \"\"\"\n",
        "    # Compute pairwise distance matrices (batch_size x batch_size)\n",
        "    # p=2 means Euclidean distance\n",
        "    t_dist = torch.cdist(teacher_emb, teacher_emb, p=2)\n",
        "    s_dist = torch.cdist(student_emb, student_emb, p=2)\n",
        "\n",
        "    # Normalize distances by the mean of the non-zero elements\n",
        "    # (This makes the loss scale-invariant)\n",
        "    t_mean = t_dist[t_dist > 0].mean()\n",
        "    s_mean = s_dist[s_dist > 0].mean()\n",
        "\n",
        "    t_dist_norm = t_dist / t_mean\n",
        "    s_dist_norm = s_dist / s_mean\n",
        "\n",
        "    # The loss is the Huber loss (smooth L1) between the normalized distance matrices\n",
        "    loss = F.smooth_l1_loss(s_dist_norm, t_dist_norm)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_featuresbefore(model, x, is_teacher=False):\n",
        "    \"\"\"\n",
        "    Manually run forward pass up to the penultimate layer to get embeddings.\n",
        "    \"\"\"\n",
        "    x = x.view(-1, 784)\n",
        "\n",
        "    if isinstance(model, TeacherNet):\n",
        "        # Teacher: fc1 -> relu -> dropout -> fc2 -> relu -> dropout -> [EMBEDDING] -> fc3\n",
        "        x = F.relu(model.fc1(x))\n",
        "        x = model.dropout(x)\n",
        "        x = F.relu(model.fc2(x))\n",
        "        # We capture the features here (after 2nd ReLU, before final dropout/classifier)\n",
        "        return x\n",
        "\n",
        "    elif isinstance(model, StudentNetbefore):\n",
        "        # Student: fc1 -> relu -> fc2 -> relu -> [EMBEDDING] -> fc3\n",
        "        x = F.relu(model.fc1(x))\n",
        "        x = F.relu(model.fc2(x))\n",
        "        return x\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def train_student_rkd(student, teacher, train_loader, epochs=10, lr=0.001, beta=1.0):\n",
        "    \"\"\"\n",
        "    Train student using RKD (Distance) + Cross Entropy.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval() # Freeze teacher\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"Training RKD Student (Beta={beta})...\")\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_rkd_loss = 0\n",
        "        total_task_loss = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. Get Teacher Embeddings\n",
        "            with torch.no_grad():\n",
        "                teacher_emb = get_featuresbefore(teacher, data, is_teacher=True)\n",
        "\n",
        "            # 2. Get Student Embeddings and Logits\n",
        "            student_emb = get_featuresbefore(student, data, is_teacher=False)\n",
        "            student_logits = student.fc3(student_emb)\n",
        "\n",
        "            # 3. Calculate Losses\n",
        "            task_loss = F.cross_entropy(student_logits, target)\n",
        "            rkd_loss_val = rkd_distance_loss(student_emb, teacher_emb)\n",
        "\n",
        "            # Combined Loss\n",
        "            loss = task_loss + (beta * rkd_loss_val)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_rkd_loss += rkd_loss_val.item()\n",
        "            total_task_loss += task_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_rkd = total_rkd_loss / len(train_loader)\n",
        "        print(f\"RKD Epoch {epoch+1}/{epochs} | Total: {avg_loss:.4f} | RKD: {avg_rkd:.4f} | Task: {total_task_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "_ic74BcEcOG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from modelsbefore import TeacherNet, StudentNetbefore, distillation_loss\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# ------------------------ LOADING THE DATA ------------------------------\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# assuming new session so loading MNIST dataset from scratch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# data loading with jittering (up to 2 pixels in any direction)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=0, translate=(2/28, 2/28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# ---------------- LOADING THE TRAINED MODELS ----------------------------\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "teacher_checkpoint_path = '/content/drive/MyDrive/Distillations/teacher_model.pth'\n",
        "student_distilled_checkpoint_path = '/content/drive/MyDrive/Distillations/student_distilled_model.pth'\n",
        "student_normal_checkpoint_path = '/content/drive/MyDrive/Distillations/student_normal_model.pth'\n",
        "fitnet_student_checkpoint_path = '/content/drive/MyDrive/Distillations/fitnet_student_model.pth'\n",
        "student_rkd_checkpoint_path = '/content/drive/MyDrive/Distillations/student_rkd_model.pth'\n",
        "\n",
        "\n",
        "\n",
        "# teacher\n",
        "checkpoint = torch.load(teacher_checkpoint_path)\n",
        "teacher = TeacherNet(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
        "teacher.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "# student KD\n",
        "checkpoint = torch.load(student_distilled_checkpoint_path)\n",
        "student_distilled_before = StudentNetbefore().to(device)\n",
        "student_distilled_before.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# student regular\n",
        "checkpoint = torch.load(student_normal_checkpoint_path)\n",
        "student_normal_before = StudentNetbefore().to(device)\n",
        "student_normal_before.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# student fitnet\n",
        "checkpoint = torch.load(fitnet_student_checkpoint_path)\n",
        "fitnet_student_before = FitNetStudentbefore().to(device)\n",
        "fitnet_student_before.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# rkd\n",
        "checkpoint = torch.load(student_rkd_checkpoint_path)\n",
        "student_rkd_before = StudentNetbefore().to(device)\n",
        "student_rkd_before.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# ---------------- Models Errors Evaluations ----------------------------\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate all models\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATING ALL MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "teacher_acc, teacher_err = evaluate(teacher, test_loader)\n",
        "print(f\"✓ Teacher evaluated: {teacher_acc:.2f}% accuracy ({teacher_err} errors)\")\n",
        "\n",
        "student_normal_acc, student_normal_err = evaluate(student_normal_before, test_loader)\n",
        "print(f\"✓ Student (normal) evaluated: {student_normal_acc:.2f}% accuracy ({student_normal_err} errors)\")\n",
        "\n",
        "student_distilled_acc, student_distilled_err = evaluate(student_distilled_before, test_loader)\n",
        "print(f\"✓ Student (distilled) evaluated: {student_distilled_acc:.2f}% accuracy ({student_distilled_err} errors)\")\n",
        "\n",
        "fitnet_acc, fitnet_err = evaluate(fitnet_student_before, test_loader)\n",
        "print(f\"✓ FitNet evaluated: {fitnet_acc:.2f}% accuracy ({fitnet_err} errors)\")\n",
        "\n",
        "student_rkd_acc, student_rkd_err = evaluate(student_rkd_before, test_loader)\n",
        "print(f\"✓ RKD Student evaluated: {student_rkd_acc:.2f}% accuracy ({student_rkd_err} errors)\")\n",
        "\n",
        "\n",
        "# Results summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Teacher (2x1200 + dropout):        {teacher_err:3d} test errors ({teacher_acc:.2f}%)\")\n",
        "print(f\"Student normal (2x800):            {student_normal_err:3d} test errors ({student_normal_acc:.2f}%)\")\n",
        "print(f\"Student distilled (2x800, T=20):   {student_distilled_err:3d} test errors ({student_distilled_acc:.2f}%)\")\n",
        "print(f\"FitNet (4x300, T=20):              {fitnet_err:3d} test errors ({fitnet_acc:.2f}%)\")\n",
        "#add for rkd\n",
        "print(f\"RKD (2x800):                       {student_rkd_err:3d} test errors ({student_rkd_acc:.2f}%)\")\n",
        "print(\"\\nPaper reported (Hinton et al.):\")\n",
        "print(\"Teacher:           67 test errors\")\n",
        "print(\"Student normal:   146 test errors\")\n",
        "print(\"Student distilled: 74 test errors\")\n",
        "print(\"\\nPaper reported (FitNets):\")\n",
        "print(\"FitNet:            51 test errors\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# ---------------- Models Inference Time ----------------------------\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Inference speed test\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INFERENCE SPEED TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "images, labels = next(iter(test_loader))\n",
        "sample = images[:1].to(device)  # Single sample image\n",
        "label = labels[:1]  # Corresponding label\n",
        "print(f'label is {label}')\n",
        "\n",
        "# get sample label\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def benchmark(model, name, num_samples=10000, num_trials=5):\n",
        "    model.eval()\n",
        "    params = count_params(model)\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(100):\n",
        "            model(sample)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        # Multiple trials\n",
        "        for trial in range(num_trials):\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            start = time.time()\n",
        "            for _ in range(num_samples):\n",
        "                model(sample)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            times.append((time.time() - start) * 1000 / num_samples)\n",
        "\n",
        "    mean_ms = sum(times) / len(times)\n",
        "    std_ms = (sum((t - mean_ms)**2 for t in times) / len(times))**0.5\n",
        "    print(f\"{name:30s}: {mean_ms:.3f} ± {std_ms:.3f} ms/sample  ({params:,} params)\")\n",
        "    return mean_ms, params\n",
        "\n",
        "t_time, t_params = benchmark(teacher, \"Teacher (2x1200)\")\n",
        "s_time, s_params = benchmark(student_normal_before, \"Student Normal (2x800)\")\n",
        "d_time, d_params = benchmark(student_distilled_before, \"Student Distilled (2x800)\")\n",
        "f_time, f_params = benchmark(fitnet_student_before, \"FitNet (4x300)\")\n",
        "r_time, r_params = benchmark(student_rkd_before, \"RKD (2x800)\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"COMPRESSION & SPEED COMPARISON\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Student Normal vs Teacher:\")\n",
        "print(f\"  Speedup: {t_time/s_time:.2f}x faster\")\n",
        "print(f\"  Compression: {s_params/t_params*100:.1f}% params\")\n",
        "\n",
        "print(f\"\\nStudent Distilled vs Teacher:\")\n",
        "print(f\"  Speedup: {t_time/d_time:.2f}x faster\")\n",
        "print(f\"  Compression: {d_params/t_params*100:.1f}% params\")\n",
        "\n",
        "print(f\"\\nFitNet vs Teacher:\")\n",
        "print(f\"  Speedup: {t_time/f_time:.2f}x faster\")\n",
        "print(f\"  Compression: {f_params/t_params*100:.1f}% params ({t_params/f_params:.1f}x smaller)\")\n",
        "\n",
        "print(f\"\\nFitNet vs Student Distilled:\")\n",
        "print(f\"  Speedup: {d_time/f_time:.2f}x\")\n",
        "print(f\"  Compression: {f_params/d_params*100:.1f}% params\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aysur7Ucn5l",
        "outputId": "09712993-88de-4044-85e0-7fbd5e3d2732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.61MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 132kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.26MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.74MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATING ALL MODELS\n",
            "============================================================\n",
            "✓ Teacher evaluated: 98.95% accuracy (105 errors)\n",
            "✓ Student (normal) evaluated: 98.77% accuracy (123 errors)\n",
            "✓ Student (distilled) evaluated: 98.85% accuracy (115 errors)\n",
            "✓ FitNet evaluated: 98.83% accuracy (117 errors)\n",
            "✓ RKD Student evaluated: 98.82% accuracy (118 errors)\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Teacher (2x1200 + dropout):        105 test errors (98.95%)\n",
            "Student normal (2x800):            123 test errors (98.77%)\n",
            "Student distilled (2x800, T=20):   115 test errors (98.85%)\n",
            "FitNet (4x300, T=20):              117 test errors (98.83%)\n",
            "RKD (2x800):                       118 test errors (98.82%)\n",
            "\n",
            "Paper reported (Hinton et al.):\n",
            "Teacher:           67 test errors\n",
            "Student normal:   146 test errors\n",
            "Student distilled: 74 test errors\n",
            "\n",
            "Paper reported (FitNets):\n",
            "FitNet:            51 test errors\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "INFERENCE SPEED TEST\n",
            "============================================================\n",
            "label is tensor([7])\n",
            "Teacher (2x1200)              : 0.183 ± 0.026 ms/sample  (2,395,210 params)\n",
            "Student Normal (2x800)        : 0.157 ± 0.025 ms/sample  (1,276,810 params)\n",
            "Student Distilled (2x800)     : 0.138 ± 0.002 ms/sample  (1,276,810 params)\n",
            "FitNet (4x300)                : 0.252 ± 0.058 ms/sample  (509,410 params)\n",
            "RKD (2x800)                   : 0.157 ± 0.020 ms/sample  (1,276,810 params)\n",
            "\n",
            "------------------------------------------------------------\n",
            "COMPRESSION & SPEED COMPARISON\n",
            "------------------------------------------------------------\n",
            "Student Normal vs Teacher:\n",
            "  Speedup: 1.17x faster\n",
            "  Compression: 53.3% params\n",
            "\n",
            "Student Distilled vs Teacher:\n",
            "  Speedup: 1.33x faster\n",
            "  Compression: 53.3% params\n",
            "\n",
            "FitNet vs Teacher:\n",
            "  Speedup: 0.73x faster\n",
            "  Compression: 21.3% params (4.7x smaller)\n",
            "\n",
            "FitNet vs Student Distilled:\n",
            "  Speedup: 0.55x\n",
            "  Compression: 39.9% params\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## load architectures after"
      ],
      "metadata": {
        "id": "zWIrIbyZcJsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile modelsafter.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TeacherNet(nn.Module):\n",
        "    \"\"\"Large teacher network: 2 hidden layers of 1200 ReLU units\"\"\"\n",
        "    def __init__(self, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 1200)\n",
        "        self.fc2 = nn.Linear(1200, 1200)\n",
        "        self.fc3 = nn.Linear(1200, 10)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        logits = self.fc3(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "\n",
        "class StudentNetafter(nn.Module):\n",
        "    \"\"\"Small student network: 2 hidden layers of 800 ReLU units\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 800)\n",
        "        self.bn1 = nn.BatchNorm1d(800)      # NEW: BatchNorm after first linear\n",
        "        self.fc2 = nn.Linear(800, 800)\n",
        "        self.bn2 = nn.BatchNorm1d(800)      # NEW: BatchNorm after second linear\n",
        "        self.fc3 = nn.Linear(800, 10)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))   # fc1 → bn1 → relu\n",
        "        identity = x                         # NEW: Save for residual\n",
        "        x = F.relu(self.bn2(self.fc2(x))) + identity  # fc2 → bn2 → relu → ADD identity\n",
        "        logits = self.fc3(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "\n",
        "def distillation_loss(student_logits, teacher_soft_targets, hard_targets,\n",
        "                      temperature, alpha=0.5):\n",
        "    \"\"\"\n",
        "    Combined loss for distillation.\n",
        "\n",
        "    Args:\n",
        "        student_logits: Raw outputs from student model\n",
        "        teacher_soft_targets: Soft probabilities from teacher (at temperature T)\n",
        "        hard_targets: Ground truth labels\n",
        "        temperature: Temperature for distillation\n",
        "        alpha: Weight for hard target loss (1-alpha is weight for soft targets)\n",
        "\n",
        "    Returns:\n",
        "        Combined loss\n",
        "    \"\"\"\n",
        "    # Soft target loss: KL divergence between student and teacher (both at temperature T)\n",
        "    student_soft = F.log_softmax(student_logits / temperature, dim=1)\n",
        "    soft_loss = F.kl_div(student_soft, teacher_soft_targets, reduction='batchmean')\n",
        "\n",
        "    # Scale by T^2 as per paper (gradients scale as 1/T^2)\n",
        "    soft_loss = soft_loss * (temperature ** 2)\n",
        "\n",
        "    # Hard target loss: Standard cross-entropy (at temperature 1)\n",
        "    hard_loss = F.cross_entropy(student_logits, hard_targets)\n",
        "\n",
        "    # Weighted combination\n",
        "    return alpha * hard_loss + (1 - alpha) * soft_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a00fd9-e44e-4fe7-f2f0-fd437e47f52f",
        "id": "sXtYLnHtgIb-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing modelsafter.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################## 2 ########################\n",
        "\n",
        "import torch\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from modelsafter import TeacherNet, StudentNetafter, distillation_loss\n",
        "\n",
        "\n",
        "def train_teacher(model, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train the large teacher model with dropout regularization\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, logits = model(data)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Teacher Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_student_normal(model, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Train student model normally (baseline)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            _, logits = model(data)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Student (normal) Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_student_distilled(student, teacher, train_loader, temperature=20,\n",
        "                           alpha=0.1, epochs=10, lr=0.001):\n",
        "    \"\"\"Train student model using knowledge distillation\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()  # Teacher is frozen\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher's soft targets at temperature T\n",
        "            with torch.no_grad():\n",
        "                teacher_soft_targets, _ = teacher(data, temperature=temperature)\n",
        "\n",
        "            # Get student outputs\n",
        "            _, student_logits = student(data)\n",
        "\n",
        "            # Compute distillation loss\n",
        "            loss = distillation_loss(student_logits, teacher_soft_targets,\n",
        "                                    target, temperature, alpha)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Student (distilled T={temperature}) Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    \"\"\"Evaluate model accuracy\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs, _ = model(data, temperature=1.0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    errors = total - correct\n",
        "    return accuracy, errors\n"
      ],
      "metadata": {
        "id": "yNm4R11rgIb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################## 3 ########################\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class FitNetStudentafter(nn.Module):\n",
        "    \"\"\"Thin and deep student: 4 hidden layers with ResNet + BatchNorm\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 300)\n",
        "        self.bn1 = nn.BatchNorm1d(300)\n",
        "        self.fc2 = nn.Linear(300, 300)\n",
        "        self.bn2 = nn.BatchNorm1d(300)\n",
        "        self.fc3 = nn.Linear(300, 300)\n",
        "        self.bn3 = nn.BatchNorm1d(300)\n",
        "        self.fc4 = nn.Linear(300, 300)\n",
        "        self.bn4 = nn.BatchNorm1d(300)\n",
        "        self.fc5 = nn.Linear(300, 10)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        identity = x\n",
        "        x = F.relu(self.bn2(self.fc2(x))) + identity\n",
        "\n",
        "        identity = x\n",
        "        x = F.relu(self.bn3(self.fc3(x))) + identity\n",
        "\n",
        "        identity = x\n",
        "        x = F.relu(self.bn4(self.fc4(x))) + identity\n",
        "\n",
        "        logits = self.fc5(x)\n",
        "        return F.softmax(logits / temperature, dim=1), logits\n",
        "\n",
        "    def forward_with_hint(self, x):\n",
        "        \"\"\"Return both output and guided layer activation\"\"\"\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        identity = x\n",
        "        guided = F.relu(self.bn2(self.fc2(x))) + identity\n",
        "\n",
        "        identity = guided\n",
        "        x = F.relu(self.bn3(self.fc3(guided))) + identity\n",
        "\n",
        "        identity = x\n",
        "        x = F.relu(self.bn4(self.fc4(x))) + identity\n",
        "\n",
        "        logits = self.fc5(x)\n",
        "        return logits, guided\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    \"\"\"Maps student guided layer (300) to teacher hint layer (1200)\"\"\"\n",
        "    def __init__(self, student_dim=300, teacher_dim=1200):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(student_dim, teacher_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.fc(x))\n",
        "\n",
        "\n",
        "\n",
        "def get_teacher_hint(teacher, x):\n",
        "    \"\"\"Extract teacher's first hidden layer activation (hint)\"\"\"\n",
        "    x = x.view(-1, 784)\n",
        "    hint = F.relu(teacher.fc1(x))  # First hidden layer\n",
        "    return hint\n",
        "\n",
        "\n",
        "def train_stage1_hints(student, teacher, regressor, train_loader, epochs=10, lr=0.001):\n",
        "    \"\"\"Stage 1: Train student (up to guided layer) + regressor to match teacher hint\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    regressor = regressor.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    # Only optimize student layers up to guided + regressor\n",
        "    optimizer = optim.Adam(list(student.parameters()) + list(regressor.parameters()), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    regressor.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, _ in train_loader:\n",
        "            data = data.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get teacher hint (first hidden layer)\n",
        "            with torch.no_grad():\n",
        "                teacher_hint = get_teacher_hint(teacher, data)\n",
        "\n",
        "            # Get student guided layer and pass through regressor\n",
        "            _, student_guided = student.forward_with_hint(data)\n",
        "            student_prediction = regressor(student_guided)\n",
        "\n",
        "            # L2 loss between regressor output and teacher hint\n",
        "            loss = F.mse_loss(student_prediction, teacher_hint)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Stage 1 Epoch {epoch+1}/{epochs}, Hint Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def train_stage2_kd(student, teacher, train_loader, temperature=20, alpha=0.1, epochs=10, lr=0.001):\n",
        "    \"\"\"Stage 2: Standard KD training (reuse from original code)\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval()\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for data, target in train_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Teacher soft targets\n",
        "            with torch.no_grad():\n",
        "                teacher_soft, _ = teacher(data, temperature=temperature)\n",
        "\n",
        "            # Student outputs\n",
        "            _, student_logits = student(data)\n",
        "\n",
        "            # Distillation loss\n",
        "            student_soft = F.log_softmax(student_logits / temperature, dim=1)\n",
        "            soft_loss = F.kl_div(student_soft, teacher_soft, reduction='batchmean') * (temperature ** 2)\n",
        "            hard_loss = F.cross_entropy(student_logits, target)\n",
        "            loss = alpha * hard_loss + (1 - alpha) * soft_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Stage 2 Epoch {epoch+1}/{epochs}, KD Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n",
        "\n",
        "\n",
        "def train_fitnet(teacher, train_loader, epochs_stage1=10, epochs_stage2=20, lr=0.001, temp=20):\n",
        "    \"\"\"Complete FitNet training: Stage 1 (hints) → Stage 2 (KD)\"\"\"\n",
        "    print(\"\\n[FitNet Stage 1] Training with hints from teacher...\")\n",
        "    student = FitNetStudentafter()\n",
        "    regressor = Regressor(student_dim=300, teacher_dim=1200)\n",
        "\n",
        "    student = train_stage1_hints(student, teacher, regressor, train_loader,\n",
        "                                 epochs=epochs_stage1, lr=lr)\n",
        "\n",
        "    print(\"\\n[FitNet Stage 2] Knowledge distillation...\")\n",
        "    student = train_stage2_kd(student, teacher, train_loader,\n",
        "                             temperature=temp, alpha=0.1, epochs=epochs_stage2, lr=lr)\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "6m0zeXnAdVnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from modelsafter import TeacherNet, StudentNetafter\n",
        "\n",
        "# --- 1. Define Helper Functions Locally to Avoid Import Errors ---\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy.\n",
        "    (Defined locally to ensure it works even if not in models.py)\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # Support both models that return (output, feature) and just (output)\n",
        "            out = model(data, temperature=1.0)\n",
        "            if isinstance(out, tuple):\n",
        "                outputs = out[0]\n",
        "            else:\n",
        "                outputs = out\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    errors = total - correct\n",
        "    return accuracy, errors\n",
        "\n",
        "def rkd_distance_loss(student_emb, teacher_emb):\n",
        "    \"\"\"\n",
        "    Relational Knowledge Distillation - Distance Loss\n",
        "    Forces student to mimic the pairwise distances found in the teacher's embedding space.\n",
        "    \"\"\"\n",
        "    # Compute pairwise distance matrices (batch_size x batch_size)\n",
        "    # p=2 means Euclidean distance\n",
        "    t_dist = torch.cdist(teacher_emb, teacher_emb, p=2)\n",
        "    s_dist = torch.cdist(student_emb, student_emb, p=2)\n",
        "\n",
        "    # Normalize distances by the mean of the non-zero elements\n",
        "    # (This makes the loss scale-invariant)\n",
        "    t_mean = t_dist[t_dist > 0].mean()\n",
        "    s_mean = s_dist[s_dist > 0].mean()\n",
        "\n",
        "    t_dist_norm = t_dist / t_mean\n",
        "    s_dist_norm = s_dist / s_mean\n",
        "\n",
        "    # The loss is the Huber loss (smooth L1) between the normalized distance matrices\n",
        "    loss = F.smooth_l1_loss(s_dist_norm, t_dist_norm)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_featuresafter(model, x, is_teacher=False):\n",
        "    x = x.view(-1, 784)\n",
        "\n",
        "    if isinstance(model, TeacherNet):\n",
        "        x = F.relu(model.fc1(x))\n",
        "        x = model.dropout(x)\n",
        "        x = F.relu(model.fc2(x))\n",
        "        return x                        # Teacher unchanged\n",
        "\n",
        "    elif isinstance(model, StudentNetafter):\n",
        "        x = F.relu(model.bn1(model.fc1(x)))      # fc1 → bn1 → relu\n",
        "        identity = x                              # Save for residual\n",
        "        x = F.relu(model.bn2(model.fc2(x))) + identity  # fc2 → bn2 → relu + skip\n",
        "        return x                                  # Return pre-fc3 embeddings\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def train_student_rkd(student, teacher, train_loader, epochs=10, lr=0.001, beta=1.0):\n",
        "    \"\"\"\n",
        "    Train student using RKD (Distance) + Cross Entropy.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    student = student.to(device)\n",
        "    teacher = teacher.to(device)\n",
        "    teacher.eval() # Freeze teacher\n",
        "\n",
        "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"Training RKD Student (Beta={beta})...\")\n",
        "\n",
        "    student.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_rkd_loss = 0\n",
        "        total_task_loss = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1. Get Teacher Embeddings\n",
        "            with torch.no_grad():\n",
        "                teacher_emb = get_featuresafter(teacher, data, is_teacher=True)\n",
        "\n",
        "            # 2. Get Student Embeddings and Logits\n",
        "            student_emb = get_featuresafter(student, data, is_teacher=False)\n",
        "            student_logits = student.fc3(student_emb)\n",
        "\n",
        "            # 3. Calculate Losses\n",
        "            task_loss = F.cross_entropy(student_logits, target)\n",
        "            rkd_loss_val = rkd_distance_loss(student_emb, teacher_emb)\n",
        "\n",
        "            # Combined Loss\n",
        "            loss = task_loss + (beta * rkd_loss_val)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_rkd_loss += rkd_loss_val.item()\n",
        "            total_task_loss += task_loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        avg_rkd = total_rkd_loss / len(train_loader)\n",
        "        print(f\"RKD Epoch {epoch+1}/{epochs} | Total: {avg_loss:.4f} | RKD: {avg_rkd:.4f} | Task: {total_task_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    return student\n"
      ],
      "metadata": {
        "id": "sjEazPewgYUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from modelsafter import TeacherNet, StudentNetafter, distillation_loss\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# ------------------------ LOADING THE DATA ------------------------------\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# assuming new session so loading MNIST dataset from scratch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "# data loading with jittering (up to 2 pixels in any direction)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=0, translate=(2/28, 2/28)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "transform_test = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# ---------------- LOADING THE TRAINED MODELS ----------------------------\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "teacher_checkpoint_path = '/content/drive/MyDrive/Distillations/teacher_model.pth'\n",
        "student_distilled_checkpoint_path = '/content/drive/MyDrive/Distillations/student_distilled_model_after_algo_change.pth'\n",
        "student_normal_checkpoint_path = '/content/drive/MyDrive/Distillations/student_normal_model_after_algo_change.pth'\n",
        "fitnet_student_checkpoint_path = '/content/drive/MyDrive/Distillations/fitnet_student_model_after_algo_change.pth'\n",
        "student_rkd_checkpoint_path = '/content/drive/MyDrive/Distillations/student_rkd_model_after_algo_change.pth'\n",
        "\n",
        "\n",
        "\n",
        "# teacher\n",
        "checkpoint = torch.load(teacher_checkpoint_path)\n",
        "teacher = TeacherNet(dropout_rate=checkpoint['dropout_rate']).to(device)\n",
        "teacher.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "# student KD\n",
        "checkpoint = torch.load(student_distilled_checkpoint_path)\n",
        "student_distilled_after = StudentNetafter().to(device)\n",
        "student_distilled_after.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# student regular\n",
        "checkpoint = torch.load(student_normal_checkpoint_path)\n",
        "student_normal_after = StudentNetafter().to(device)\n",
        "student_normal_after.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# student fitnet\n",
        "checkpoint = torch.load(fitnet_student_checkpoint_path)\n",
        "fitnet_student_after = FitNetStudentafter().to(device)\n",
        "fitnet_student_after.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# rkd\n",
        "checkpoint = torch.load(student_rkd_checkpoint_path)\n",
        "student_rkd_after = StudentNetafter().to(device)\n",
        "student_rkd_after.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# ---------------- Models Errors Evaluations ----------------------------\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate all models\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATING ALL MODELS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "teacher_acc, teacher_err = evaluate(teacher, test_loader)\n",
        "print(f\"✓ Teacher evaluated: {teacher_acc:.2f}% accuracy ({teacher_err} errors)\")\n",
        "\n",
        "student_normal_acc, student_normal_err = evaluate(student_normal_after, test_loader)\n",
        "print(f\"✓ Student (normal) evaluated: {student_normal_acc:.2f}% accuracy ({student_normal_err} errors)\")\n",
        "\n",
        "student_distilled_acc, student_distilled_err = evaluate(student_distilled_after, test_loader)\n",
        "print(f\"✓ Student (distilled) evaluated: {student_distilled_acc:.2f}% accuracy ({student_distilled_err} errors)\")\n",
        "\n",
        "fitnet_acc, fitnet_err = evaluate(fitnet_student_after, test_loader)\n",
        "print(f\"✓ FitNet evaluated: {fitnet_acc:.2f}% accuracy ({fitnet_err} errors)\")\n",
        "\n",
        "student_rkd_acc, student_rkd_err = evaluate(student_rkd_after, test_loader)\n",
        "print(f\"✓ RKD Student evaluated: {student_rkd_acc:.2f}% accuracy ({student_rkd_err} errors)\")\n",
        "\n",
        "\n",
        "# Results summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Teacher (2x1200 + dropout):        {teacher_err:3d} test errors ({teacher_acc:.2f}%)\")\n",
        "print(f\"Student normal (2x800):            {student_normal_err:3d} test errors ({student_normal_acc:.2f}%)\")\n",
        "print(f\"Student distilled (2x800, T=20):   {student_distilled_err:3d} test errors ({student_distilled_acc:.2f}%)\")\n",
        "print(f\"FitNet (4x300, T=20):              {fitnet_err:3d} test errors ({fitnet_acc:.2f}%)\")\n",
        "#add for rkd\n",
        "print(f\"RKD (2x800):                       {student_rkd_err:3d} test errors ({student_rkd_acc:.2f}%)\")\n",
        "print(\"\\nPaper reported (Hinton et al.):\")\n",
        "print(\"Teacher:           67 test errors\")\n",
        "print(\"Student normal:   146 test errors\")\n",
        "print(\"Student distilled: 74 test errors\")\n",
        "print(\"\\nPaper reported (FitNets):\")\n",
        "print(\"FitNet:            51 test errors\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------\n",
        "# ---------------- Models Inference Time ----------------------------\n",
        "# ------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Inference speed test\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INFERENCE SPEED TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "images, labels = next(iter(test_loader))\n",
        "sample = images[:1].to(device)  # Single sample image\n",
        "label = labels[:1]  # Corresponding label\n",
        "print(f'label is {label}')\n",
        "\n",
        "# get sample label\n",
        "\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def benchmark(model, name, num_samples=10000, num_trials=5):\n",
        "    model.eval()\n",
        "    params = count_params(model)\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Warmup\n",
        "        for _ in range(100):\n",
        "            model(sample)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "\n",
        "        # Multiple trials\n",
        "        for trial in range(num_trials):\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            start = time.time()\n",
        "            for _ in range(num_samples):\n",
        "                model(sample)\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.synchronize()\n",
        "            times.append((time.time() - start) * 1000 / num_samples)\n",
        "\n",
        "    mean_ms = sum(times) / len(times)\n",
        "    std_ms = (sum((t - mean_ms)**2 for t in times) / len(times))**0.5\n",
        "    print(f\"{name:30s}: {mean_ms:.3f} ± {std_ms:.3f} ms/sample  ({params:,} params)\")\n",
        "    return mean_ms, params\n",
        "\n",
        "t_time, t_params = benchmark(teacher, \"Teacher (2x1200)\")\n",
        "s_time, s_params = benchmark(student_normal_after, \"Student Normal (2x800)\")\n",
        "d_time, d_params = benchmark(student_distilled_after, \"Student Distilled (2x800)\")\n",
        "f_time, f_params = benchmark(fitnet_student_after, \"FitNet (4x300)\")\n",
        "r_time, r_params = benchmark(student_rkd_after, \"RKD (2x800)\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"COMPRESSION & SPEED COMPARISON\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"Student Normal vs Teacher:\")\n",
        "print(f\"  Speedup: {t_time/s_time:.2f}x faster\")\n",
        "print(f\"  Compression: {s_params/t_params*100:.1f}% params\")\n",
        "\n",
        "print(f\"\\nStudent Distilled vs Teacher:\")\n",
        "print(f\"  Speedup: {t_time/d_time:.2f}x faster\")\n",
        "print(f\"  Compression: {d_params/t_params*100:.1f}% params\")\n",
        "\n",
        "print(f\"\\nFitNet vs Teacher:\")\n",
        "print(f\"  Speedup: {t_time/f_time:.2f}x faster\")\n",
        "print(f\"  Compression: {f_params/t_params*100:.1f}% params ({t_params/f_params:.1f}x smaller)\")\n",
        "\n",
        "print(f\"\\nFitNet vs Student Distilled:\")\n",
        "print(f\"  Speedup: {d_time/f_time:.2f}x\")\n",
        "print(f\"  Compression: {f_params/d_params*100:.1f}% params\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff5ACSZvgk--",
        "outputId": "9987ed34-7024-48d9-afd3-b9867576e3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n",
            "\n",
            "============================================================\n",
            "EVALUATING ALL MODELS\n",
            "============================================================\n",
            "✓ Teacher evaluated: 98.95% accuracy (105 errors)\n",
            "✓ Student (normal) evaluated: 98.73% accuracy (127 errors)\n",
            "✓ Student (distilled) evaluated: 98.92% accuracy (108 errors)\n",
            "✓ FitNet evaluated: 98.80% accuracy (120 errors)\n",
            "✓ RKD Student evaluated: 99.08% accuracy (92 errors)\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "Teacher (2x1200 + dropout):        105 test errors (98.95%)\n",
            "Student normal (2x800):            127 test errors (98.73%)\n",
            "Student distilled (2x800, T=20):   108 test errors (98.92%)\n",
            "FitNet (4x300, T=20):              120 test errors (98.80%)\n",
            "RKD (2x800):                        92 test errors (99.08%)\n",
            "\n",
            "Paper reported (Hinton et al.):\n",
            "Teacher:           67 test errors\n",
            "Student normal:   146 test errors\n",
            "Student distilled: 74 test errors\n",
            "\n",
            "Paper reported (FitNets):\n",
            "FitNet:            51 test errors\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "INFERENCE SPEED TEST\n",
            "============================================================\n",
            "label is tensor([7])\n",
            "Teacher (2x1200)              : 0.203 ± 0.037 ms/sample  (2,395,210 params)\n",
            "Student Normal (2x800)        : 0.441 ± 0.152 ms/sample  (1,280,010 params)\n",
            "Student Distilled (2x800)     : 0.295 ± 0.020 ms/sample  (1,280,010 params)\n",
            "FitNet (4x300)                : 0.513 ± 0.037 ms/sample  (511,810 params)\n",
            "RKD (2x800)                   : 0.292 ± 0.027 ms/sample  (1,280,010 params)\n",
            "\n",
            "------------------------------------------------------------\n",
            "COMPRESSION & SPEED COMPARISON\n",
            "------------------------------------------------------------\n",
            "Student Normal vs Teacher:\n",
            "  Speedup: 0.46x faster\n",
            "  Compression: 53.4% params\n",
            "\n",
            "Student Distilled vs Teacher:\n",
            "  Speedup: 0.69x faster\n",
            "  Compression: 53.4% params\n",
            "\n",
            "FitNet vs Teacher:\n",
            "  Speedup: 0.39x faster\n",
            "  Compression: 21.4% params (4.7x smaller)\n",
            "\n",
            "FitNet vs Student Distilled:\n",
            "  Speedup: 0.57x\n",
            "  Compression: 40.0% params\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test"
      ],
      "metadata": {
        "id": "wnROi7TbdSu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.contingency_tables import mcnemar\n",
        "import numpy as np\n",
        "\n",
        "def get_all_predictions(model, loader, device):\n",
        "    \"\"\"Get predictions for entire test set\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Handle if model returns tuple (output, features)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # Take only the logits\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "def mcnemar_comparison(preds1, preds2, true_labels, name1, name2):\n",
        "    \"\"\"Run McNemar's test between two models\"\"\"\n",
        "    correct1 = (preds1 == true_labels)\n",
        "    correct2 = (preds2 == true_labels)\n",
        "\n",
        "    # Contingency table: [both wrong, model1 only right]\n",
        "    #                     [model2 only right, both right]\n",
        "    b = np.sum(correct1 & ~correct2)  # model1 right, model2 wrong\n",
        "    c = np.sum(~correct1 & correct2)  # model1 wrong, model2 right\n",
        "\n",
        "    table = [[0, b], [c, 0]]\n",
        "\n",
        "    result = mcnemar(table, exact=False, correction=True)\n",
        "\n",
        "    print(f\"\\n{name1} vs {name2}:\")\n",
        "    print(f\"  {name1} correct, {name2} wrong: {b}\")\n",
        "    print(f\"  {name1} wrong, {name2} correct: {c}\")\n",
        "    print(f\"  McNemar statistic: {result.statistic:.3f}\")\n",
        "    print(f\"  p-value: {result.pvalue:.4f}\")\n",
        "    print(f\"  Significant: {'Yes' if result.pvalue < 0.05 else 'No'}\")\n",
        "\n",
        "    return result.pvalue"
      ],
      "metadata": {
        "id": "ZAdNnKAsWnp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"McNEMAR'S TEST: ALGORITHMIC CHANGE IMPACT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Compare Baseline before vs after algo change\n",
        "baseline_before_preds, labels = get_all_predictions(student_normal_before, test_loader, device)\n",
        "baseline_after_preds, _ = get_all_predictions(student_normal_after, test_loader, device)\n",
        "mcnemar_comparison(baseline_before_preds, baseline_after_preds, labels,\n",
        "                   \"Baseline (before)\", \"Baseline (after)\")\n",
        "\n",
        "# Compare Vanilla KD before vs after algo change\n",
        "vanilla_before_preds, _ = get_all_predictions(student_distilled_before, test_loader, device)\n",
        "vanilla_after_preds, _ = get_all_predictions(student_distilled_after, test_loader, device)\n",
        "mcnemar_comparison(vanilla_before_preds, vanilla_after_preds, labels,\n",
        "                   \"Vanilla KD (before)\", \"Vanilla KD (after)\")\n",
        "\n",
        "# Compare FitNet before vs after\n",
        "fitnet_before_preds, _ = get_all_predictions(fitnet_student_before, test_loader, device)\n",
        "fitnet_after_preds, _ = get_all_predictions(fitnet_student_after, test_loader, device)\n",
        "mcnemar_comparison(fitnet_before_preds, fitnet_after_preds, labels,\n",
        "                   \"FitNet (before)\", \"FitNet (after)\")\n",
        "\n",
        "# Compare RKD before vs after\n",
        "rkd_before_preds, _ = get_all_predictions(student_rkd_before, test_loader, device)\n",
        "rkd_after_preds, _ = get_all_predictions(student_rkd_after, test_loader, device)\n",
        "mcnemar_comparison(rkd_before_preds, rkd_after_preds, labels,\n",
        "                   \"RKD (before)\", \"RKD (after)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icwe6vYQWoTz",
        "outputId": "72d76a10-3c48-4372-c9e8-e82da0bb6ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "McNEMAR'S TEST: ALGORITHMIC CHANGE IMPACT\n",
            "============================================================\n",
            "\n",
            "Baseline (before) vs Baseline (after):\n",
            "  Baseline (before) correct, Baseline (after) wrong: 77\n",
            "  Baseline (before) wrong, Baseline (after) correct: 73\n",
            "  McNemar statistic: 0.060\n",
            "  p-value: 0.8065\n",
            "  Significant: No\n",
            "\n",
            "Vanilla KD (before) vs Vanilla KD (after):\n",
            "  Vanilla KD (before) correct, Vanilla KD (after) wrong: 16\n",
            "  Vanilla KD (before) wrong, Vanilla KD (after) correct: 23\n",
            "  McNemar statistic: 0.923\n",
            "  p-value: 0.3367\n",
            "  Significant: No\n",
            "\n",
            "FitNet (before) vs FitNet (after):\n",
            "  FitNet (before) correct, FitNet (after) wrong: 25\n",
            "  FitNet (before) wrong, FitNet (after) correct: 22\n",
            "  McNemar statistic: 0.085\n",
            "  p-value: 0.7705\n",
            "  Significant: No\n",
            "\n",
            "RKD (before) vs RKD (after):\n",
            "  RKD (before) correct, RKD (after) wrong: 34\n",
            "  RKD (before) wrong, RKD (after) correct: 60\n",
            "  McNemar statistic: 6.649\n",
            "  p-value: 0.0099\n",
            "  Significant: Yes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.009921504538268757)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}